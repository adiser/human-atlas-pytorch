{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretrainedmodels\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from transforms import *\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Channel Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {'polynet':{\n",
    "                   'input_size': 331,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'resnet34':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'senet154':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'nasnetamobile':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    },\n",
    "                 'bninception':{\n",
    "                   'input_size': 299,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    },\n",
    "                 'xception':{\n",
    "                   'input_size': 299,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "class AtlasData(Dataset):\n",
    "    def __init__(self, split, train = True, model = 'bninception'):\n",
    "        self.split = split\n",
    "        self.train = train\n",
    "        self.train_str = 'train' if self.train else 'test'\n",
    "        self.text_file = 'data/atlas_{}_split_{}.txt'.format(self.train_str, self.split)\n",
    "        \n",
    "        self.data = [[y for y in x.strip().split(' ')] for x in open(self.text_file, 'r').readlines()]\n",
    "        self.imgs = [x[0] for x in self.data]\n",
    "        self.labels = [[int(p) for p in x[1:]] for x in self.data]\n",
    "        \n",
    "        self.input_size = model_configs[model]['input_size']\n",
    "        self.input_mean = model_configs[model]['input_mean']\n",
    "        self.input_std = model_configs[model]['input_std']\n",
    "        \n",
    "        self.transforms = transforms.Compose([GroupRandomRotate(360),\n",
    "                                              GroupScale(self.input_size),\n",
    "                                              Stack(roll=False),\n",
    "                                              ToTorchFormatTensor(div=True),\n",
    "                                              transforms.Normalize(self.input_mean, self.input_std),\n",
    "                                            ])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_image_stack(self, image_id):\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "        absolute_paths = [\"data/train/{}_{}.png\".format(image_id, color) for color in colors]\n",
    "        images = [Image.open(path).convert('L') for path in absolute_paths]\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_id = self.imgs[i]\n",
    "        image = self.load_image_stack(image_id)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        label = self.labels[i]\n",
    "        label_arr = np.zeros(28, dtype = np.float32)\n",
    "        [np.put(label_arr, x, 1) for x in label]\n",
    "        \n",
    "        label_arr = torch.from_numpy(label_arr)\n",
    "        \n",
    "        return image, label_arr, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct RGBY Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rgby_model(model):\n",
    "    modules = list(model.modules())\n",
    "    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "    conv_layer = modules[first_conv_idx]\n",
    "    container = modules[first_conv_idx - 1]\n",
    "\n",
    "    params = [x.clone() for x in conv_layer.parameters()]\n",
    "    kernel_size = params[0].size()\n",
    "    new_kernel_size = kernel_size[:1] + (4, ) + kernel_size[2:]\n",
    "    new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "\n",
    "    new_conv = nn.Conv2d(4, conv_layer.out_channels,\n",
    "                         conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                         bias=True if len(params) == 2 else False)\n",
    "    \n",
    "    new_conv.weight.data = new_kernels\n",
    "    if len(params) == 2:\n",
    "        new_conv.bias.data = params[1].data \n",
    "    layer_name = list(container.state_dict().keys())[0][:-7] \n",
    "\n",
    "    setattr(container, layer_name, new_conv)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/ipykernel_launcher.py:101: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/ipykernel_launcher.py:112: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Iteration [1/1289], Loss: 0.1055 (0.1055), Elapsed Time 0.6941\n",
      "Epoch [5], Iteration [101/1289], Loss: 0.0924 (0.0880), Elapsed Time 72.5745\n",
      "Epoch [5], Iteration [201/1289], Loss: 0.0971 (0.0867), Elapsed Time 143.4503\n",
      "Epoch [5], Iteration [301/1289], Loss: 0.0730 (0.0862), Elapsed Time 212.0373\n",
      "Epoch [5], Iteration [401/1289], Loss: 0.1198 (0.0863), Elapsed Time 278.6110\n",
      "Epoch [5], Iteration [501/1289], Loss: 0.0817 (0.0870), Elapsed Time 346.4746\n",
      "Epoch [5], Iteration [601/1289], Loss: 0.1187 (0.0872), Elapsed Time 414.5736\n",
      "Epoch [5], Iteration [701/1289], Loss: 0.0860 (0.0874), Elapsed Time 481.5677\n",
      "Epoch [5], Iteration [801/1289], Loss: 0.1073 (0.0873), Elapsed Time 550.4969\n",
      "Epoch [5], Iteration [901/1289], Loss: 0.0859 (0.0870), Elapsed Time 617.7292\n",
      "Epoch [5], Iteration [1001/1289], Loss: 0.1071 (0.0873), Elapsed Time 685.4109\n",
      "Epoch [5], Iteration [1101/1289], Loss: 0.1035 (0.0872), Elapsed Time 752.6763\n",
      "Epoch [5], Iteration [1201/1289], Loss: 0.0736 (0.0870), Elapsed Time 819.3295\n",
      "Average Loss: 0.08693667501211166\n",
      "Testing 0/10449: Loss 0.056923408061265945\n",
      "Testing 1000/10449: Loss 0.08317729830741882\n",
      "Testing 2000/10449: Loss 0.08353206515312195\n",
      "Testing 3000/10449: Loss 0.08487153053283691\n",
      "Testing 4000/10449: Loss 0.08483096212148666\n",
      "Testing 5000/10449: Loss 0.08559886366128922\n",
      "Testing 6000/10449: Loss 0.08616876602172852\n",
      "Testing 7000/10449: Loss 0.08591310679912567\n",
      "Testing 8000/10449: Loss 0.08623203635215759\n",
      "Testing 9000/10449: Loss 0.08622987568378448\n",
      "Testing 10000/10449: Loss 0.08662191778421402\n",
      "Avg Loss 0.08658281713724136\n",
      "Score 0.4615015981712718\n",
      "Epoch [6], Iteration [1/1289], Loss: 0.0889 (0.0889), Elapsed Time 0.8694\n",
      "Epoch [6], Iteration [101/1289], Loss: 0.0712 (0.0812), Elapsed Time 87.1539\n",
      "Epoch [6], Iteration [201/1289], Loss: 0.0768 (0.0808), Elapsed Time 175.1743\n",
      "Epoch [6], Iteration [301/1289], Loss: 0.0613 (0.0801), Elapsed Time 263.7297\n",
      "Epoch [6], Iteration [401/1289], Loss: 0.0685 (0.0801), Elapsed Time 351.6190\n",
      "Epoch [6], Iteration [501/1289], Loss: 0.0980 (0.0807), Elapsed Time 428.7265\n",
      "Epoch [6], Iteration [601/1289], Loss: 0.0835 (0.0814), Elapsed Time 499.2379\n",
      "Epoch [6], Iteration [701/1289], Loss: 0.0881 (0.0815), Elapsed Time 568.9927\n",
      "Epoch [6], Iteration [801/1289], Loss: 0.0862 (0.0814), Elapsed Time 649.9667\n",
      "Epoch [6], Iteration [901/1289], Loss: 0.0729 (0.0817), Elapsed Time 736.4315\n",
      "Epoch [6], Iteration [1001/1289], Loss: 0.0872 (0.0816), Elapsed Time 821.7523\n",
      "Epoch [6], Iteration [1101/1289], Loss: 0.0423 (0.0814), Elapsed Time 892.2370\n",
      "Epoch [6], Iteration [1201/1289], Loss: 0.0742 (0.0817), Elapsed Time 966.1207\n",
      "Average Loss: 0.08161931484937668\n",
      "Testing 0/10449: Loss 0.04234974458813667\n",
      "Testing 1000/10449: Loss 0.08455197513103485\n",
      "Testing 2000/10449: Loss 0.08413306623697281\n",
      "Testing 3000/10449: Loss 0.08413968980312347\n",
      "Testing 4000/10449: Loss 0.08468251675367355\n",
      "Testing 5000/10449: Loss 0.08506828546524048\n",
      "Testing 6000/10449: Loss 0.08506546914577484\n",
      "Testing 7000/10449: Loss 0.08486001938581467\n",
      "Testing 8000/10449: Loss 0.08538945764303207\n",
      "Testing 9000/10449: Loss 0.08519942313432693\n",
      "Testing 10000/10449: Loss 0.08548431843519211\n",
      "Avg Loss 0.08542127907276154\n",
      "Score 0.4992068685303355\n",
      "Epoch [7], Iteration [1/1289], Loss: 0.0846 (0.0846), Elapsed Time 0.5759\n",
      "Epoch [7], Iteration [101/1289], Loss: 0.0833 (0.0748), Elapsed Time 57.5867\n",
      "Epoch [7], Iteration [201/1289], Loss: 0.0698 (0.0748), Elapsed Time 114.1787\n",
      "Epoch [7], Iteration [301/1289], Loss: 0.0794 (0.0749), Elapsed Time 172.6111\n",
      "Epoch [7], Iteration [401/1289], Loss: 0.0983 (0.0760), Elapsed Time 230.9368\n",
      "Epoch [7], Iteration [501/1289], Loss: 0.1319 (0.0759), Elapsed Time 291.2898\n",
      "Epoch [7], Iteration [601/1289], Loss: 0.0501 (0.0758), Elapsed Time 349.1915\n",
      "Epoch [7], Iteration [701/1289], Loss: 0.0986 (0.0760), Elapsed Time 407.4258\n",
      "Epoch [7], Iteration [801/1289], Loss: 0.0641 (0.0763), Elapsed Time 465.7071\n",
      "Epoch [7], Iteration [901/1289], Loss: 0.0568 (0.0768), Elapsed Time 523.8751\n",
      "Epoch [7], Iteration [1001/1289], Loss: 0.0767 (0.0772), Elapsed Time 582.7178\n",
      "Epoch [7], Iteration [1101/1289], Loss: 0.0712 (0.0774), Elapsed Time 641.4583\n",
      "Epoch [7], Iteration [1201/1289], Loss: 0.0873 (0.0776), Elapsed Time 699.0811\n",
      "Average Loss: 0.07766422629356384\n",
      "Testing 0/10449: Loss 0.05697315186262131\n",
      "Testing 1000/10449: Loss 0.08459348231554031\n",
      "Testing 2000/10449: Loss 0.08451635390520096\n",
      "Testing 3000/10449: Loss 0.08486831188201904\n",
      "Testing 4000/10449: Loss 0.08497562259435654\n",
      "Testing 5000/10449: Loss 0.0849849283695221\n",
      "Testing 6000/10449: Loss 0.08539525419473648\n",
      "Testing 7000/10449: Loss 0.08535408228635788\n",
      "Testing 8000/10449: Loss 0.08600503951311111\n",
      "Testing 9000/10449: Loss 0.08604636788368225\n",
      "Testing 10000/10449: Loss 0.08647215366363525\n",
      "Avg Loss 0.08634528517723083\n",
      "Score 0.5173280929163492\n",
      "Epoch [8], Iteration [1/1289], Loss: 0.0859 (0.0859), Elapsed Time 0.5855\n",
      "Epoch [8], Iteration [101/1289], Loss: 0.0612 (0.0753), Elapsed Time 57.1061\n",
      "Epoch [8], Iteration [201/1289], Loss: 0.0643 (0.0744), Elapsed Time 114.4460\n",
      "Epoch [8], Iteration [301/1289], Loss: 0.0618 (0.0737), Elapsed Time 171.7051\n",
      "Epoch [8], Iteration [401/1289], Loss: 0.0727 (0.0734), Elapsed Time 229.4628\n",
      "Epoch [8], Iteration [501/1289], Loss: 0.0663 (0.0732), Elapsed Time 287.2269\n",
      "Epoch [8], Iteration [601/1289], Loss: 0.0501 (0.0730), Elapsed Time 344.9441\n",
      "Epoch [8], Iteration [701/1289], Loss: 0.1043 (0.0730), Elapsed Time 405.1968\n",
      "Epoch [8], Iteration [801/1289], Loss: 0.0639 (0.0731), Elapsed Time 464.2148\n",
      "Epoch [8], Iteration [901/1289], Loss: 0.0850 (0.0728), Elapsed Time 522.7371\n",
      "Epoch [8], Iteration [1001/1289], Loss: 0.0539 (0.0729), Elapsed Time 583.7575\n",
      "Epoch [8], Iteration [1101/1289], Loss: 0.0815 (0.0732), Elapsed Time 642.8140\n",
      "Epoch [8], Iteration [1201/1289], Loss: 0.0888 (0.0732), Elapsed Time 702.3097\n",
      "Average Loss: 0.0733499601483345\n",
      "Testing 0/10449: Loss 0.02939945086836815\n",
      "Testing 1000/10449: Loss 0.08296770602464676\n",
      "Testing 2000/10449: Loss 0.08266035467386246\n",
      "Testing 3000/10449: Loss 0.0831981897354126\n",
      "Testing 4000/10449: Loss 0.08353568613529205\n",
      "Testing 5000/10449: Loss 0.08347106724977493\n",
      "Testing 6000/10449: Loss 0.08388642966747284\n",
      "Testing 7000/10449: Loss 0.08382217586040497\n",
      "Testing 8000/10449: Loss 0.08445677161216736\n",
      "Testing 9000/10449: Loss 0.0842585638165474\n",
      "Testing 10000/10449: Loss 0.08438073098659515\n",
      "Avg Loss 0.08442923426628113\n",
      "Score 0.5377779464636656\n",
      "Epoch [9], Iteration [1/1289], Loss: 0.0522 (0.0522), Elapsed Time 0.5829\n",
      "Epoch [9], Iteration [101/1289], Loss: 0.0551 (0.0647), Elapsed Time 58.4924\n",
      "Epoch [9], Iteration [201/1289], Loss: 0.0596 (0.0660), Elapsed Time 117.9677\n",
      "Epoch [9], Iteration [301/1289], Loss: 0.0413 (0.0672), Elapsed Time 174.5490\n",
      "Epoch [9], Iteration [401/1289], Loss: 0.0679 (0.0682), Elapsed Time 231.0072\n",
      "Epoch [9], Iteration [501/1289], Loss: 0.0854 (0.0684), Elapsed Time 288.1739\n",
      "Epoch [9], Iteration [601/1289], Loss: 0.0963 (0.0684), Elapsed Time 344.9589\n",
      "Epoch [9], Iteration [701/1289], Loss: 0.0811 (0.0688), Elapsed Time 401.6909\n",
      "Epoch [9], Iteration [801/1289], Loss: 0.0552 (0.0688), Elapsed Time 458.5764\n",
      "Epoch [9], Iteration [901/1289], Loss: 0.0625 (0.0689), Elapsed Time 519.5486\n",
      "Epoch [9], Iteration [1001/1289], Loss: 0.0416 (0.0685), Elapsed Time 575.0265\n",
      "Epoch [9], Iteration [1101/1289], Loss: 0.0672 (0.0687), Elapsed Time 630.5731\n",
      "Epoch [9], Iteration [1201/1289], Loss: 0.0663 (0.0691), Elapsed Time 685.7786\n",
      "Average Loss: 0.0692456066608429\n",
      "Testing 0/10449: Loss 0.03390363231301308\n",
      "Testing 1000/10449: Loss 0.0813627690076828\n",
      "Testing 2000/10449: Loss 0.08105017244815826\n",
      "Testing 3000/10449: Loss 0.0824674591422081\n",
      "Testing 4000/10449: Loss 0.08286046981811523\n",
      "Testing 5000/10449: Loss 0.08277459442615509\n",
      "Testing 6000/10449: Loss 0.08296860009431839\n",
      "Testing 7000/10449: Loss 0.0824299082159996\n",
      "Testing 8000/10449: Loss 0.08303376287221909\n",
      "Testing 9000/10449: Loss 0.08305913954973221\n",
      "Testing 10000/10449: Loss 0.08342587947845459\n",
      "Avg Loss 0.08334789425134659\n",
      "Score 0.5694318189840393\n",
      "Epoch [10], Iteration [1/1289], Loss: 0.0691 (0.0691), Elapsed Time 0.6111\n",
      "Epoch [10], Iteration [101/1289], Loss: 0.0337 (0.0627), Elapsed Time 59.7827\n",
      "Epoch [10], Iteration [201/1289], Loss: 0.0792 (0.0628), Elapsed Time 120.9128\n",
      "Epoch [10], Iteration [301/1289], Loss: 0.0695 (0.0632), Elapsed Time 178.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Iteration [401/1289], Loss: 0.0299 (0.0629), Elapsed Time 240.5912\n",
      "Epoch [10], Iteration [501/1289], Loss: 0.0621 (0.0636), Elapsed Time 300.7936\n",
      "Epoch [10], Iteration [601/1289], Loss: 0.0801 (0.0639), Elapsed Time 360.1015\n",
      "Epoch [10], Iteration [701/1289], Loss: 0.0920 (0.0640), Elapsed Time 419.5753\n",
      "Epoch [10], Iteration [801/1289], Loss: 0.0545 (0.0641), Elapsed Time 480.4198\n",
      "Epoch [10], Iteration [901/1289], Loss: 0.0655 (0.0646), Elapsed Time 542.0334\n",
      "Epoch [10], Iteration [1001/1289], Loss: 0.0695 (0.0647), Elapsed Time 600.6816\n",
      "Epoch [10], Iteration [1101/1289], Loss: 0.0360 (0.0648), Elapsed Time 659.7306\n",
      "Epoch [10], Iteration [1201/1289], Loss: 0.0707 (0.0649), Elapsed Time 718.5028\n",
      "Average Loss: 0.06500355899333954\n",
      "Testing 0/10449: Loss 0.050384100526571274\n",
      "Testing 1000/10449: Loss 0.0825512483716011\n",
      "Testing 2000/10449: Loss 0.08099889755249023\n",
      "Testing 3000/10449: Loss 0.0825745165348053\n",
      "Testing 4000/10449: Loss 0.08266692608594894\n",
      "Testing 5000/10449: Loss 0.08318821340799332\n",
      "Testing 6000/10449: Loss 0.08361361175775528\n",
      "Testing 7000/10449: Loss 0.083541139960289\n",
      "Testing 8000/10449: Loss 0.08411431312561035\n",
      "Testing 9000/10449: Loss 0.0843028649687767\n",
      "Testing 10000/10449: Loss 0.08465729653835297\n",
      "Avg Loss 0.08462267369031906\n",
      "Score 0.569093979510549\n",
      "Epoch [11], Iteration [1/1289], Loss: 0.0336 (0.0336), Elapsed Time 0.5911\n",
      "Epoch [11], Iteration [101/1289], Loss: 0.0795 (0.0587), Elapsed Time 58.4118\n",
      "Epoch [11], Iteration [201/1289], Loss: 0.0448 (0.0582), Elapsed Time 115.3608\n",
      "Epoch [11], Iteration [301/1289], Loss: 0.0575 (0.0579), Elapsed Time 172.5196\n",
      "Epoch [11], Iteration [401/1289], Loss: 0.0529 (0.0585), Elapsed Time 230.7286\n",
      "Epoch [11], Iteration [501/1289], Loss: 0.0821 (0.0592), Elapsed Time 287.6060\n",
      "Epoch [11], Iteration [601/1289], Loss: 0.0564 (0.0601), Elapsed Time 344.3422\n",
      "Epoch [11], Iteration [701/1289], Loss: 0.0531 (0.0603), Elapsed Time 401.1560\n",
      "Epoch [11], Iteration [801/1289], Loss: 0.0539 (0.0606), Elapsed Time 457.9500\n",
      "Epoch [11], Iteration [901/1289], Loss: 0.0767 (0.0608), Elapsed Time 514.6480\n",
      "Epoch [11], Iteration [1001/1289], Loss: 0.0779 (0.0611), Elapsed Time 573.0713\n",
      "Epoch [11], Iteration [1101/1289], Loss: 0.0591 (0.0611), Elapsed Time 632.1013\n",
      "Epoch [11], Iteration [1201/1289], Loss: 0.0625 (0.0611), Elapsed Time 693.3318\n",
      "Average Loss: 0.06126200780272484\n",
      "Testing 0/10449: Loss 0.05602921172976494\n",
      "Testing 1000/10449: Loss 0.08077506721019745\n",
      "Testing 2000/10449: Loss 0.08203168213367462\n",
      "Testing 3000/10449: Loss 0.08304620534181595\n",
      "Testing 4000/10449: Loss 0.08298859000205994\n",
      "Testing 5000/10449: Loss 0.0830627903342247\n",
      "Testing 6000/10449: Loss 0.08295431733131409\n",
      "Testing 7000/10449: Loss 0.08306928724050522\n",
      "Testing 8000/10449: Loss 0.08377835899591446\n",
      "Testing 9000/10449: Loss 0.08346738666296005\n",
      "Testing 10000/10449: Loss 0.08385083824396133\n",
      "Avg Loss 0.08385806530714035\n",
      "Score 0.5759377282390664\n",
      "Epoch [12], Iteration [1/1289], Loss: 0.0878 (0.0878), Elapsed Time 0.7183\n",
      "Epoch [12], Iteration [101/1289], Loss: 0.1120 (0.0585), Elapsed Time 60.1072\n",
      "Epoch [12], Iteration [201/1289], Loss: 0.0375 (0.0562), Elapsed Time 118.2136\n",
      "Epoch [12], Iteration [301/1289], Loss: 0.0598 (0.0558), Elapsed Time 174.9714\n",
      "Epoch [12], Iteration [401/1289], Loss: 0.0419 (0.0550), Elapsed Time 232.4535\n",
      "Epoch [12], Iteration [501/1289], Loss: 0.0665 (0.0552), Elapsed Time 289.8732\n",
      "Epoch [12], Iteration [601/1289], Loss: 0.0776 (0.0553), Elapsed Time 347.3796\n",
      "Epoch [12], Iteration [701/1289], Loss: 0.0611 (0.0558), Elapsed Time 405.1011\n",
      "Epoch [12], Iteration [801/1289], Loss: 0.0482 (0.0561), Elapsed Time 462.0991\n",
      "Epoch [12], Iteration [901/1289], Loss: 0.0421 (0.0562), Elapsed Time 520.1146\n",
      "Epoch [12], Iteration [1001/1289], Loss: 0.0439 (0.0567), Elapsed Time 579.0270\n",
      "Epoch [12], Iteration [1101/1289], Loss: 0.0800 (0.0568), Elapsed Time 636.1780\n",
      "Epoch [12], Iteration [1201/1289], Loss: 0.0517 (0.0569), Elapsed Time 692.8317\n",
      "Average Loss: 0.05685983598232269\n",
      "Testing 0/10449: Loss 0.06691451370716095\n",
      "Testing 1000/10449: Loss 0.08721532672643661\n",
      "Testing 2000/10449: Loss 0.08626464754343033\n",
      "Testing 3000/10449: Loss 0.08770427852869034\n",
      "Testing 4000/10449: Loss 0.08805524557828903\n",
      "Testing 5000/10449: Loss 0.0880303606390953\n",
      "Testing 6000/10449: Loss 0.08811600506305695\n",
      "Testing 7000/10449: Loss 0.0877353772521019\n",
      "Testing 8000/10449: Loss 0.08808893710374832\n",
      "Testing 9000/10449: Loss 0.08810798823833466\n",
      "Testing 10000/10449: Loss 0.08832815289497375\n",
      "Avg Loss 0.08844878524541855\n",
      "Score 0.5902135497955506\n",
      "Epoch [13], Iteration [1/1289], Loss: 0.0497 (0.0497), Elapsed Time 0.5816\n",
      "Epoch [13], Iteration [101/1289], Loss: 0.0349 (0.0498), Elapsed Time 57.0579\n",
      "Epoch [13], Iteration [201/1289], Loss: 0.0567 (0.0519), Elapsed Time 114.2538\n",
      "Epoch [13], Iteration [301/1289], Loss: 0.0592 (0.0514), Elapsed Time 171.0032\n",
      "Epoch [13], Iteration [401/1289], Loss: 0.0616 (0.0510), Elapsed Time 230.2664\n",
      "Epoch [13], Iteration [501/1289], Loss: 0.0427 (0.0514), Elapsed Time 292.7972\n",
      "Epoch [13], Iteration [601/1289], Loss: 0.0656 (0.0513), Elapsed Time 354.3207\n",
      "Epoch [13], Iteration [701/1289], Loss: 0.0416 (0.0518), Elapsed Time 417.2656\n",
      "Epoch [13], Iteration [801/1289], Loss: 0.0663 (0.0518), Elapsed Time 479.0429\n",
      "Epoch [13], Iteration [901/1289], Loss: 0.0473 (0.0522), Elapsed Time 540.6753\n",
      "Epoch [13], Iteration [1001/1289], Loss: 0.0761 (0.0524), Elapsed Time 602.2772\n",
      "Epoch [13], Iteration [1101/1289], Loss: 0.0441 (0.0526), Elapsed Time 664.4797\n",
      "Epoch [13], Iteration [1201/1289], Loss: 0.0645 (0.0528), Elapsed Time 727.9563\n",
      "Average Loss: 0.05276418849825859\n",
      "Testing 0/10449: Loss 0.06858044862747192\n",
      "Testing 1000/10449: Loss 0.08470911532640457\n",
      "Testing 2000/10449: Loss 0.08557235449552536\n",
      "Testing 3000/10449: Loss 0.08669959008693695\n",
      "Testing 4000/10449: Loss 0.08677919209003448\n",
      "Testing 5000/10449: Loss 0.08720502257347107\n",
      "Testing 6000/10449: Loss 0.0874052569270134\n",
      "Testing 7000/10449: Loss 0.08791004121303558\n",
      "Testing 8000/10449: Loss 0.08846218138933182\n",
      "Testing 9000/10449: Loss 0.08880224823951721\n",
      "Testing 10000/10449: Loss 0.0888175368309021\n",
      "Avg Loss 0.08869881927967072\n",
      "Score 0.6218922018631663\n",
      "Epoch [14], Iteration [1/1289], Loss: 0.0667 (0.0667), Elapsed Time 0.6650\n",
      "Epoch [14], Iteration [101/1289], Loss: 0.0537 (0.0489), Elapsed Time 58.8910\n",
      "Epoch [14], Iteration [201/1289], Loss: 0.0556 (0.0488), Elapsed Time 116.0204\n",
      "Epoch [14], Iteration [301/1289], Loss: 0.0424 (0.0484), Elapsed Time 173.4991\n",
      "Epoch [14], Iteration [401/1289], Loss: 0.0476 (0.0486), Elapsed Time 230.3727\n",
      "Epoch [14], Iteration [501/1289], Loss: 0.0523 (0.0486), Elapsed Time 287.1609\n",
      "Epoch [14], Iteration [601/1289], Loss: 0.0498 (0.0490), Elapsed Time 344.5161\n",
      "Epoch [14], Iteration [701/1289], Loss: 0.0440 (0.0494), Elapsed Time 407.2353\n",
      "Epoch [14], Iteration [801/1289], Loss: 0.0489 (0.0494), Elapsed Time 471.6949\n",
      "Epoch [14], Iteration [901/1289], Loss: 0.0565 (0.0496), Elapsed Time 537.0537\n",
      "Epoch [14], Iteration [1001/1289], Loss: 0.0667 (0.0497), Elapsed Time 603.0036\n",
      "Epoch [14], Iteration [1101/1289], Loss: 0.0438 (0.0496), Elapsed Time 668.6968\n",
      "Epoch [14], Iteration [1201/1289], Loss: 0.0515 (0.0495), Elapsed Time 735.2277\n",
      "Average Loss: 0.04960044473409653\n",
      "Testing 0/10449: Loss 0.0520995669066906\n",
      "Testing 1000/10449: Loss 0.08642792701721191\n",
      "Testing 2000/10449: Loss 0.08573263138532639\n",
      "Testing 3000/10449: Loss 0.08723436295986176\n",
      "Testing 4000/10449: Loss 0.08697927743196487\n",
      "Testing 5000/10449: Loss 0.08672139048576355\n",
      "Testing 6000/10449: Loss 0.0874273031949997\n",
      "Testing 7000/10449: Loss 0.0877968817949295\n",
      "Testing 8000/10449: Loss 0.0883006751537323\n",
      "Testing 9000/10449: Loss 0.0881013497710228\n",
      "Testing 10000/10449: Loss 0.08807128667831421\n",
      "Avg Loss 0.0879388228058815\n",
      "Score 0.5987334687928001\n",
      "Epoch [15], Iteration [1/1289], Loss: 0.0418 (0.0418), Elapsed Time 0.6564\n",
      "Epoch [15], Iteration [101/1289], Loss: 0.0251 (0.0447), Elapsed Time 58.9821\n",
      "Epoch [15], Iteration [201/1289], Loss: 0.0426 (0.0444), Elapsed Time 117.2530\n",
      "Epoch [15], Iteration [301/1289], Loss: 0.0392 (0.0457), Elapsed Time 175.2703\n",
      "Epoch [15], Iteration [401/1289], Loss: 0.0610 (0.0455), Elapsed Time 232.7998\n",
      "Epoch [15], Iteration [501/1289], Loss: 0.0219 (0.0450), Elapsed Time 291.5376\n",
      "Epoch [15], Iteration [601/1289], Loss: 0.0439 (0.0448), Elapsed Time 350.1873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Iteration [701/1289], Loss: 0.0385 (0.0448), Elapsed Time 408.5679\n",
      "Epoch [15], Iteration [801/1289], Loss: 0.0388 (0.0449), Elapsed Time 466.9868\n",
      "Epoch [15], Iteration [901/1289], Loss: 0.0401 (0.0450), Elapsed Time 524.2537\n",
      "Epoch [15], Iteration [1001/1289], Loss: 0.0275 (0.0453), Elapsed Time 581.3808\n",
      "Epoch [15], Iteration [1101/1289], Loss: 0.0478 (0.0454), Elapsed Time 638.6042\n",
      "Epoch [15], Iteration [1201/1289], Loss: 0.0374 (0.0457), Elapsed Time 695.7257\n",
      "Average Loss: 0.045806944370269775\n",
      "Testing 0/10449: Loss 0.073348268866539\n",
      "Testing 1000/10449: Loss 0.08964750170707703\n",
      "Testing 2000/10449: Loss 0.08834327012300491\n",
      "Testing 3000/10449: Loss 0.08951395750045776\n",
      "Testing 4000/10449: Loss 0.08932550251483917\n",
      "Testing 5000/10449: Loss 0.08918073028326035\n",
      "Testing 6000/10449: Loss 0.08889801800251007\n",
      "Testing 7000/10449: Loss 0.08915096521377563\n",
      "Testing 8000/10449: Loss 0.08950851112604141\n",
      "Testing 9000/10449: Loss 0.08950302004814148\n",
      "Testing 10000/10449: Loss 0.09000128507614136\n",
      "Avg Loss 0.08992048352956772\n",
      "Score 0.5955697125577025\n",
      "Epoch [16], Iteration [1/1289], Loss: 0.0493 (0.0493), Elapsed Time 0.5865\n",
      "Epoch [16], Iteration [101/1289], Loss: 0.0433 (0.0394), Elapsed Time 58.1973\n",
      "Epoch [16], Iteration [201/1289], Loss: 0.0239 (0.0397), Elapsed Time 116.0769\n",
      "Epoch [16], Iteration [301/1289], Loss: 0.0536 (0.0404), Elapsed Time 173.5065\n",
      "Epoch [16], Iteration [401/1289], Loss: 0.0313 (0.0405), Elapsed Time 230.6991\n",
      "Epoch [16], Iteration [501/1289], Loss: 0.0290 (0.0412), Elapsed Time 287.9400\n",
      "Epoch [16], Iteration [601/1289], Loss: 0.0478 (0.0416), Elapsed Time 345.4699\n",
      "Epoch [16], Iteration [701/1289], Loss: 0.0243 (0.0416), Elapsed Time 402.6764\n",
      "Epoch [16], Iteration [801/1289], Loss: 0.0487 (0.0415), Elapsed Time 459.8352\n",
      "Epoch [16], Iteration [901/1289], Loss: 0.0250 (0.0417), Elapsed Time 517.0276\n",
      "Epoch [16], Iteration [1001/1289], Loss: 0.0453 (0.0418), Elapsed Time 574.4986\n",
      "Epoch [16], Iteration [1101/1289], Loss: 0.0622 (0.0421), Elapsed Time 631.9411\n",
      "Epoch [16], Iteration [1201/1289], Loss: 0.0519 (0.0422), Elapsed Time 689.0700\n",
      "Average Loss: 0.04214395582675934\n",
      "Testing 0/10449: Loss 0.05108112469315529\n",
      "Testing 1000/10449: Loss 0.0906277522444725\n",
      "Testing 2000/10449: Loss 0.08983200043439865\n",
      "Testing 3000/10449: Loss 0.09000114351511002\n",
      "Testing 4000/10449: Loss 0.09028636664152145\n",
      "Testing 5000/10449: Loss 0.09000414609909058\n",
      "Testing 6000/10449: Loss 0.08986635506153107\n",
      "Testing 7000/10449: Loss 0.08990222960710526\n",
      "Testing 8000/10449: Loss 0.09013296663761139\n",
      "Testing 9000/10449: Loss 0.08985554426908493\n",
      "Testing 10000/10449: Loss 0.09037204831838608\n",
      "Avg Loss 0.09035612642765045\n",
      "Score 0.6085003807609256\n",
      "Epoch [17], Iteration [1/1289], Loss: 0.0606 (0.0606), Elapsed Time 0.6128\n",
      "Epoch [17], Iteration [101/1289], Loss: 0.0339 (0.0363), Elapsed Time 59.5047\n",
      "Epoch [17], Iteration [201/1289], Loss: 0.0444 (0.0372), Elapsed Time 118.4351\n",
      "Epoch [17], Iteration [301/1289], Loss: 0.0359 (0.0370), Elapsed Time 178.6660\n",
      "Epoch [17], Iteration [401/1289], Loss: 0.0304 (0.0371), Elapsed Time 235.1868\n",
      "Epoch [17], Iteration [501/1289], Loss: 0.0345 (0.0371), Elapsed Time 294.6746\n",
      "Epoch [17], Iteration [601/1289], Loss: 0.0695 (0.0376), Elapsed Time 352.1522\n",
      "Epoch [17], Iteration [701/1289], Loss: 0.0217 (0.0378), Elapsed Time 409.9902\n",
      "Epoch [17], Iteration [801/1289], Loss: 0.0260 (0.0382), Elapsed Time 466.8276\n",
      "Epoch [17], Iteration [901/1289], Loss: 0.0301 (0.0384), Elapsed Time 525.8261\n",
      "Epoch [17], Iteration [1001/1289], Loss: 0.0308 (0.0386), Elapsed Time 583.7801\n",
      "Epoch [17], Iteration [1101/1289], Loss: 0.0765 (0.0388), Elapsed Time 641.7443\n",
      "Epoch [17], Iteration [1201/1289], Loss: 0.0400 (0.0389), Elapsed Time 699.6075\n",
      "Average Loss: 0.03916844353079796\n",
      "Testing 0/10449: Loss 0.10219454765319824\n",
      "Testing 1000/10449: Loss 0.09368108958005905\n",
      "Testing 2000/10449: Loss 0.09039361029863358\n",
      "Testing 3000/10449: Loss 0.09040243178606033\n",
      "Testing 4000/10449: Loss 0.08955135196447372\n",
      "Testing 5000/10449: Loss 0.08981376886367798\n",
      "Testing 6000/10449: Loss 0.09011437743902206\n",
      "Testing 7000/10449: Loss 0.08992417901754379\n",
      "Testing 8000/10449: Loss 0.09103196114301682\n",
      "Testing 9000/10449: Loss 0.09145878255367279\n",
      "Testing 10000/10449: Loss 0.09195458889007568\n",
      "Avg Loss 0.0918436199426651\n",
      "Score 0.6439730412455484\n",
      "Epoch [18], Iteration [1/1289], Loss: 0.0249 (0.0249), Elapsed Time 0.6099\n",
      "Epoch [18], Iteration [101/1289], Loss: 0.0513 (0.0337), Elapsed Time 60.5613\n",
      "Epoch [18], Iteration [201/1289], Loss: 0.0223 (0.0336), Elapsed Time 120.8972\n",
      "Epoch [18], Iteration [301/1289], Loss: 0.0410 (0.0336), Elapsed Time 182.0672\n",
      "Epoch [18], Iteration [401/1289], Loss: 0.0474 (0.0340), Elapsed Time 242.5017\n",
      "Epoch [18], Iteration [501/1289], Loss: 0.0773 (0.0347), Elapsed Time 300.5913\n",
      "Epoch [18], Iteration [601/1289], Loss: 0.0523 (0.0348), Elapsed Time 356.9654\n",
      "Epoch [18], Iteration [701/1289], Loss: 0.0195 (0.0347), Elapsed Time 414.0092\n",
      "Epoch [18], Iteration [801/1289], Loss: 0.0252 (0.0349), Elapsed Time 470.9757\n",
      "Epoch [18], Iteration [901/1289], Loss: 0.0556 (0.0353), Elapsed Time 528.1830\n",
      "Epoch [18], Iteration [1001/1289], Loss: 0.0312 (0.0355), Elapsed Time 585.4374\n",
      "Epoch [18], Iteration [1101/1289], Loss: 0.0323 (0.0357), Elapsed Time 643.2944\n",
      "Epoch [18], Iteration [1201/1289], Loss: 0.0362 (0.0358), Elapsed Time 700.8501\n",
      "Average Loss: 0.035964809358119965\n",
      "Testing 0/10449: Loss 0.08131676912307739\n",
      "Testing 1000/10449: Loss 0.0919472873210907\n",
      "Testing 2000/10449: Loss 0.09131649881601334\n",
      "Testing 3000/10449: Loss 0.09318353235721588\n",
      "Testing 4000/10449: Loss 0.0924215242266655\n",
      "Testing 5000/10449: Loss 0.09260482341051102\n",
      "Testing 6000/10449: Loss 0.09261252731084824\n",
      "Testing 7000/10449: Loss 0.09333029389381409\n",
      "Testing 8000/10449: Loss 0.09396658092737198\n",
      "Testing 9000/10449: Loss 0.09403030574321747\n",
      "Testing 10000/10449: Loss 0.09446428716182709\n",
      "Avg Loss 0.09449757635593414\n",
      "Score 0.6224896318053437\n",
      "Epoch [19], Iteration [1/1289], Loss: 0.0248 (0.0248), Elapsed Time 0.6002\n",
      "Epoch [19], Iteration [101/1289], Loss: 0.0546 (0.0311), Elapsed Time 56.2114\n",
      "Epoch [19], Iteration [201/1289], Loss: 0.0220 (0.0306), Elapsed Time 111.7238\n",
      "Epoch [19], Iteration [301/1289], Loss: 0.0217 (0.0304), Elapsed Time 167.3226\n",
      "Epoch [19], Iteration [401/1289], Loss: 0.0180 (0.0307), Elapsed Time 222.9345\n",
      "Epoch [19], Iteration [501/1289], Loss: 0.0215 (0.0309), Elapsed Time 278.7832\n",
      "Epoch [19], Iteration [601/1289], Loss: 0.0239 (0.0310), Elapsed Time 334.3268\n",
      "Epoch [19], Iteration [701/1289], Loss: 0.0521 (0.0314), Elapsed Time 389.7748\n",
      "Epoch [19], Iteration [801/1289], Loss: 0.0268 (0.0319), Elapsed Time 447.6674\n",
      "Epoch [19], Iteration [901/1289], Loss: 0.0321 (0.0321), Elapsed Time 504.8026\n",
      "Epoch [19], Iteration [1001/1289], Loss: 0.0198 (0.0322), Elapsed Time 562.7506\n",
      "Epoch [19], Iteration [1101/1289], Loss: 0.0394 (0.0321), Elapsed Time 621.3409\n",
      "Epoch [19], Iteration [1201/1289], Loss: 0.0400 (0.0324), Elapsed Time 678.8990\n",
      "Average Loss: 0.0325966402888298\n",
      "Testing 0/10449: Loss 0.16614791750907898\n",
      "Testing 1000/10449: Loss 0.09244969487190247\n",
      "Testing 2000/10449: Loss 0.09250128269195557\n",
      "Testing 3000/10449: Loss 0.09395425766706467\n",
      "Testing 4000/10449: Loss 0.09421009570360184\n",
      "Testing 5000/10449: Loss 0.09411010891199112\n",
      "Testing 6000/10449: Loss 0.09426885843276978\n",
      "Testing 7000/10449: Loss 0.09462030231952667\n",
      "Testing 8000/10449: Loss 0.09525629878044128\n",
      "Testing 9000/10449: Loss 0.09606446325778961\n",
      "Testing 10000/10449: Loss 0.09608188271522522\n",
      "Avg Loss 0.0959208682179451\n",
      "Score 0.6234047110743859\n",
      "Epoch [20], Iteration [1/1289], Loss: 0.0241 (0.0241), Elapsed Time 0.6664\n",
      "Epoch [20], Iteration [101/1289], Loss: 0.0156 (0.0295), Elapsed Time 57.0904\n",
      "Epoch [20], Iteration [201/1289], Loss: 0.0297 (0.0290), Elapsed Time 112.9653\n",
      "Epoch [20], Iteration [301/1289], Loss: 0.0178 (0.0288), Elapsed Time 168.4792\n",
      "Epoch [20], Iteration [401/1289], Loss: 0.0346 (0.0291), Elapsed Time 224.2480\n",
      "Epoch [20], Iteration [501/1289], Loss: 0.0329 (0.0289), Elapsed Time 280.7371\n",
      "Epoch [20], Iteration [601/1289], Loss: 0.0380 (0.0294), Elapsed Time 338.1230\n",
      "Epoch [20], Iteration [701/1289], Loss: 0.0417 (0.0297), Elapsed Time 395.5841\n",
      "Epoch [20], Iteration [801/1289], Loss: 0.0360 (0.0301), Elapsed Time 452.6247\n",
      "Epoch [20], Iteration [901/1289], Loss: 0.0233 (0.0303), Elapsed Time 509.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Iteration [1001/1289], Loss: 0.0398 (0.0304), Elapsed Time 566.4373\n",
      "Epoch [20], Iteration [1101/1289], Loss: 0.0251 (0.0305), Elapsed Time 623.5984\n",
      "Epoch [20], Iteration [1201/1289], Loss: 0.0202 (0.0308), Elapsed Time 680.5304\n",
      "Average Loss: 0.03089461475610733\n",
      "Testing 0/10449: Loss 0.0531555600464344\n",
      "Testing 1000/10449: Loss 0.09645521640777588\n",
      "Testing 2000/10449: Loss 0.09774275869131088\n",
      "Testing 3000/10449: Loss 0.09842666983604431\n",
      "Testing 4000/10449: Loss 0.09866738319396973\n",
      "Testing 5000/10449: Loss 0.09805353730916977\n",
      "Testing 6000/10449: Loss 0.09841042757034302\n",
      "Testing 7000/10449: Loss 0.09825305640697479\n",
      "Testing 8000/10449: Loss 0.09856606274843216\n",
      "Testing 9000/10449: Loss 0.09851410984992981\n",
      "Testing 10000/10449: Loss 0.09914117306470871\n",
      "Avg Loss 0.0992145761847496\n",
      "Score 0.6434988910021302\n",
      "Epoch [21], Iteration [1/1289], Loss: 0.0350 (0.0350), Elapsed Time 0.5893\n",
      "Epoch [21], Iteration [101/1289], Loss: 0.0212 (0.0261), Elapsed Time 57.1232\n",
      "Epoch [21], Iteration [201/1289], Loss: 0.0119 (0.0256), Elapsed Time 114.1968\n",
      "Epoch [21], Iteration [301/1289], Loss: 0.0270 (0.0259), Elapsed Time 170.8444\n",
      "Epoch [21], Iteration [401/1289], Loss: 0.0175 (0.0262), Elapsed Time 227.7614\n",
      "Epoch [21], Iteration [501/1289], Loss: 0.0329 (0.0267), Elapsed Time 284.8569\n",
      "Epoch [21], Iteration [601/1289], Loss: 0.0256 (0.0271), Elapsed Time 341.3508\n",
      "Epoch [21], Iteration [701/1289], Loss: 0.0442 (0.0275), Elapsed Time 398.1043\n",
      "Epoch [21], Iteration [801/1289], Loss: 0.0310 (0.0276), Elapsed Time 455.0445\n",
      "Epoch [21], Iteration [901/1289], Loss: 0.0330 (0.0276), Elapsed Time 512.5202\n",
      "Epoch [21], Iteration [1001/1289], Loss: 0.0093 (0.0277), Elapsed Time 569.8496\n",
      "Epoch [21], Iteration [1101/1289], Loss: 0.0149 (0.0281), Elapsed Time 627.4018\n",
      "Epoch [21], Iteration [1201/1289], Loss: 0.0337 (0.0281), Elapsed Time 685.0562\n",
      "Average Loss: 0.028108246624469757\n",
      "Testing 0/10449: Loss 0.10859136283397675\n",
      "Testing 1000/10449: Loss 0.09498527646064758\n",
      "Testing 2000/10449: Loss 0.09606821089982986\n",
      "Testing 3000/10449: Loss 0.09705394506454468\n",
      "Testing 4000/10449: Loss 0.09695223718881607\n",
      "Testing 5000/10449: Loss 0.09694655984640121\n",
      "Testing 6000/10449: Loss 0.09650371223688126\n",
      "Testing 7000/10449: Loss 0.09697620570659637\n",
      "Testing 8000/10449: Loss 0.097653329372406\n",
      "Testing 9000/10449: Loss 0.09730258584022522\n",
      "Testing 10000/10449: Loss 0.09766709059476852\n",
      "Avg Loss 0.09762057662010193\n",
      "Score 0.6372169966852954\n",
      "Epoch [22], Iteration [1/1289], Loss: 0.0296 (0.0296), Elapsed Time 0.5784\n",
      "Epoch [22], Iteration [101/1289], Loss: 0.0118 (0.0233), Elapsed Time 59.2505\n",
      "Epoch [22], Iteration [201/1289], Loss: 0.0239 (0.0235), Elapsed Time 117.6485\n",
      "Epoch [22], Iteration [301/1289], Loss: 0.0349 (0.0233), Elapsed Time 175.8377\n",
      "Epoch [22], Iteration [401/1289], Loss: 0.0220 (0.0236), Elapsed Time 234.2300\n",
      "Epoch [22], Iteration [501/1289], Loss: 0.0325 (0.0241), Elapsed Time 292.2265\n",
      "Epoch [22], Iteration [601/1289], Loss: 0.0188 (0.0243), Elapsed Time 349.3814\n",
      "Epoch [22], Iteration [701/1289], Loss: 0.0254 (0.0242), Elapsed Time 406.4534\n",
      "Epoch [22], Iteration [801/1289], Loss: 0.0445 (0.0248), Elapsed Time 463.6804\n",
      "Epoch [22], Iteration [901/1289], Loss: 0.0532 (0.0249), Elapsed Time 520.7046\n",
      "Epoch [22], Iteration [1001/1289], Loss: 0.0337 (0.0251), Elapsed Time 577.2119\n",
      "Epoch [22], Iteration [1101/1289], Loss: 0.0325 (0.0252), Elapsed Time 633.8626\n",
      "Epoch [22], Iteration [1201/1289], Loss: 0.0240 (0.0252), Elapsed Time 690.2471\n",
      "Average Loss: 0.025498004630208015\n",
      "Testing 0/10449: Loss 0.09703651815652847\n",
      "Testing 1000/10449: Loss 0.10678441822528839\n",
      "Testing 2000/10449: Loss 0.1033259779214859\n",
      "Testing 3000/10449: Loss 0.10354490578174591\n",
      "Testing 4000/10449: Loss 0.10352388024330139\n",
      "Testing 5000/10449: Loss 0.10415533930063248\n",
      "Testing 6000/10449: Loss 0.10434950143098831\n",
      "Testing 7000/10449: Loss 0.1044868528842926\n",
      "Testing 8000/10449: Loss 0.10522491484880447\n",
      "Testing 9000/10449: Loss 0.10549260675907135\n",
      "Testing 10000/10449: Loss 0.10553323477506638\n",
      "Avg Loss 0.10580986738204956\n",
      "Score 0.6216419753494667\n",
      "Epoch [23], Iteration [1/1289], Loss: 0.0140 (0.0140), Elapsed Time 0.5948\n",
      "Epoch [23], Iteration [101/1289], Loss: 0.0382 (0.0234), Elapsed Time 57.9767\n",
      "Epoch [23], Iteration [201/1289], Loss: 0.0248 (0.0234), Elapsed Time 115.6826\n",
      "Epoch [23], Iteration [301/1289], Loss: 0.0142 (0.0234), Elapsed Time 173.0910\n",
      "Epoch [23], Iteration [401/1289], Loss: 0.0315 (0.0233), Elapsed Time 230.6843\n",
      "Epoch [23], Iteration [501/1289], Loss: 0.0230 (0.0233), Elapsed Time 288.0803\n",
      "Epoch [23], Iteration [601/1289], Loss: 0.0193 (0.0233), Elapsed Time 345.5077\n",
      "Epoch [23], Iteration [701/1289], Loss: 0.0371 (0.0234), Elapsed Time 403.0076\n",
      "Epoch [23], Iteration [801/1289], Loss: 0.0191 (0.0233), Elapsed Time 461.2821\n",
      "Epoch [23], Iteration [901/1289], Loss: 0.0197 (0.0234), Elapsed Time 518.7778\n",
      "Epoch [23], Iteration [1001/1289], Loss: 0.0120 (0.0236), Elapsed Time 577.6320\n",
      "Epoch [23], Iteration [1101/1289], Loss: 0.0137 (0.0237), Elapsed Time 636.3380\n",
      "Epoch [23], Iteration [1201/1289], Loss: 0.0212 (0.0240), Elapsed Time 694.2610\n",
      "Average Loss: 0.024086546152830124\n",
      "Testing 0/10449: Loss 0.060278844088315964\n",
      "Testing 1000/10449: Loss 0.10266096144914627\n",
      "Testing 2000/10449: Loss 0.10136621445417404\n",
      "Testing 3000/10449: Loss 0.10350451618432999\n",
      "Testing 4000/10449: Loss 0.10404498130083084\n",
      "Testing 5000/10449: Loss 0.10383905470371246\n",
      "Testing 6000/10449: Loss 0.10339495539665222\n",
      "Testing 7000/10449: Loss 0.10378362983465195\n",
      "Testing 8000/10449: Loss 0.10449039191007614\n",
      "Testing 9000/10449: Loss 0.10441534966230392\n",
      "Testing 10000/10449: Loss 0.10557923465967178\n",
      "Avg Loss 0.10553848743438721\n",
      "Score 0.6360472664236434\n",
      "Epoch [24], Iteration [1/1289], Loss: 0.0246 (0.0246), Elapsed Time 0.5793\n",
      "Epoch [24], Iteration [101/1289], Loss: 0.0188 (0.0204), Elapsed Time 56.3344\n",
      "Epoch [24], Iteration [201/1289], Loss: 0.0352 (0.0200), Elapsed Time 114.0605\n",
      "Epoch [24], Iteration [301/1289], Loss: 0.0363 (0.0200), Elapsed Time 170.9992\n",
      "Epoch [24], Iteration [401/1289], Loss: 0.0098 (0.0203), Elapsed Time 227.1591\n",
      "Epoch [24], Iteration [501/1289], Loss: 0.0289 (0.0205), Elapsed Time 283.8013\n",
      "Epoch [24], Iteration [601/1289], Loss: 0.0108 (0.0205), Elapsed Time 341.0959\n",
      "Epoch [24], Iteration [701/1289], Loss: 0.0208 (0.0208), Elapsed Time 398.2596\n",
      "Epoch [24], Iteration [801/1289], Loss: 0.0182 (0.0211), Elapsed Time 455.2136\n",
      "Epoch [24], Iteration [901/1289], Loss: 0.0358 (0.0216), Elapsed Time 512.9862\n",
      "Epoch [24], Iteration [1001/1289], Loss: 0.0377 (0.0216), Elapsed Time 571.2864\n",
      "Epoch [24], Iteration [1101/1289], Loss: 0.0300 (0.0219), Elapsed Time 629.0414\n",
      "Epoch [24], Iteration [1201/1289], Loss: 0.0277 (0.0221), Elapsed Time 686.7102\n",
      "Average Loss: 0.02209584228694439\n",
      "Testing 0/10449: Loss 0.05704759806394577\n",
      "Testing 1000/10449: Loss 0.1079661026597023\n",
      "Testing 2000/10449: Loss 0.10536372661590576\n",
      "Testing 3000/10449: Loss 0.10554096102714539\n",
      "Testing 4000/10449: Loss 0.10535362362861633\n",
      "Testing 5000/10449: Loss 0.10493529587984085\n",
      "Testing 6000/10449: Loss 0.10479936748743057\n",
      "Testing 7000/10449: Loss 0.10467158257961273\n",
      "Testing 8000/10449: Loss 0.10568667203187943\n",
      "Testing 9000/10449: Loss 0.10547512769699097\n",
      "Testing 10000/10449: Loss 0.10604395717382431\n",
      "Avg Loss 0.10607898980379105\n",
      "Score 0.6500070336982395\n",
      "Epoch [25], Iteration [1/1289], Loss: 0.0147 (0.0147), Elapsed Time 0.6025\n",
      "Epoch [25], Iteration [101/1289], Loss: 0.0114 (0.0168), Elapsed Time 59.1405\n",
      "Epoch [25], Iteration [201/1289], Loss: 0.0047 (0.0181), Elapsed Time 116.6346\n",
      "Epoch [25], Iteration [301/1289], Loss: 0.0134 (0.0188), Elapsed Time 173.5255\n",
      "Epoch [25], Iteration [401/1289], Loss: 0.0130 (0.0189), Elapsed Time 230.5260\n",
      "Epoch [25], Iteration [501/1289], Loss: 0.0189 (0.0194), Elapsed Time 287.2607\n",
      "Epoch [25], Iteration [601/1289], Loss: 0.0260 (0.0196), Elapsed Time 343.8017\n",
      "Epoch [25], Iteration [701/1289], Loss: 0.0241 (0.0200), Elapsed Time 400.4600\n",
      "Epoch [25], Iteration [801/1289], Loss: 0.0169 (0.0199), Elapsed Time 456.9608\n",
      "Epoch [25], Iteration [901/1289], Loss: 0.0171 (0.0201), Elapsed Time 513.0072\n",
      "Epoch [25], Iteration [1001/1289], Loss: 0.0300 (0.0202), Elapsed Time 569.3779\n",
      "Epoch [25], Iteration [1101/1289], Loss: 0.0231 (0.0204), Elapsed Time 626.0152\n",
      "Epoch [25], Iteration [1201/1289], Loss: 0.0117 (0.0205), Elapsed Time 682.7452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.020707665011286736\n",
      "Testing 0/10449: Loss 0.04068291187286377\n",
      "Testing 1000/10449: Loss 0.11050763726234436\n",
      "Testing 2000/10449: Loss 0.10879965871572495\n",
      "Testing 3000/10449: Loss 0.10889080166816711\n",
      "Testing 4000/10449: Loss 0.10852678120136261\n",
      "Testing 5000/10449: Loss 0.10794684290885925\n",
      "Testing 6000/10449: Loss 0.10780800133943558\n",
      "Testing 7000/10449: Loss 0.10836219042539597\n",
      "Testing 8000/10449: Loss 0.10963139683008194\n",
      "Testing 9000/10449: Loss 0.10951690375804901\n",
      "Testing 10000/10449: Loss 0.10946264863014221\n",
      "Avg Loss 0.10939693450927734\n",
      "Score 0.6353056753666605\n",
      "Epoch [26], Iteration [1/1289], Loss: 0.0111 (0.0111), Elapsed Time 0.6014\n",
      "Epoch [26], Iteration [101/1289], Loss: 0.0214 (0.0170), Elapsed Time 59.3872\n",
      "Epoch [26], Iteration [201/1289], Loss: 0.0118 (0.0181), Elapsed Time 116.0597\n",
      "Epoch [26], Iteration [301/1289], Loss: 0.0351 (0.0186), Elapsed Time 174.3398\n",
      "Epoch [26], Iteration [401/1289], Loss: 0.0122 (0.0185), Elapsed Time 231.3124\n",
      "Epoch [26], Iteration [501/1289], Loss: 0.0193 (0.0182), Elapsed Time 289.0748\n",
      "Epoch [26], Iteration [601/1289], Loss: 0.0144 (0.0181), Elapsed Time 346.8626\n",
      "Epoch [26], Iteration [701/1289], Loss: 0.0092 (0.0184), Elapsed Time 403.4833\n",
      "Epoch [26], Iteration [801/1289], Loss: 0.0295 (0.0186), Elapsed Time 459.7355\n",
      "Epoch [26], Iteration [901/1289], Loss: 0.0165 (0.0187), Elapsed Time 516.3854\n",
      "Epoch [26], Iteration [1001/1289], Loss: 0.0234 (0.0188), Elapsed Time 573.0247\n",
      "Epoch [26], Iteration [1101/1289], Loss: 0.0104 (0.0189), Elapsed Time 629.5323\n",
      "Epoch [26], Iteration [1201/1289], Loss: 0.0207 (0.0190), Elapsed Time 685.8342\n",
      "Average Loss: 0.019107859581708908\n",
      "Testing 0/10449: Loss 0.07595951110124588\n",
      "Testing 1000/10449: Loss 0.11331869661808014\n",
      "Testing 2000/10449: Loss 0.10976917296648026\n",
      "Testing 3000/10449: Loss 0.11189474165439606\n",
      "Testing 4000/10449: Loss 0.11105082929134369\n",
      "Testing 5000/10449: Loss 0.11034266650676727\n",
      "Testing 6000/10449: Loss 0.11131551861763\n",
      "Testing 7000/10449: Loss 0.11235750466585159\n",
      "Testing 8000/10449: Loss 0.11301954090595245\n",
      "Testing 9000/10449: Loss 0.1130644753575325\n",
      "Testing 10000/10449: Loss 0.11375097930431366\n",
      "Avg Loss 0.11395363509654999\n",
      "Score 0.6318161230080269\n",
      "Epoch [27], Iteration [1/1289], Loss: 0.0127 (0.0127), Elapsed Time 0.5794\n",
      "Epoch [27], Iteration [101/1289], Loss: 0.0145 (0.0159), Elapsed Time 60.0883\n",
      "Epoch [27], Iteration [201/1289], Loss: 0.0148 (0.0157), Elapsed Time 119.0189\n",
      "Epoch [27], Iteration [301/1289], Loss: 0.0212 (0.0161), Elapsed Time 178.7038\n",
      "Epoch [27], Iteration [401/1289], Loss: 0.0067 (0.0162), Elapsed Time 238.8381\n",
      "Epoch [27], Iteration [501/1289], Loss: 0.0140 (0.0167), Elapsed Time 297.2468\n",
      "Epoch [27], Iteration [601/1289], Loss: 0.0130 (0.0168), Elapsed Time 357.6809\n",
      "Epoch [27], Iteration [701/1289], Loss: 0.0233 (0.0170), Elapsed Time 417.9111\n",
      "Epoch [27], Iteration [801/1289], Loss: 0.0104 (0.0169), Elapsed Time 477.3274\n",
      "Epoch [27], Iteration [901/1289], Loss: 0.0259 (0.0172), Elapsed Time 536.0475\n",
      "Epoch [27], Iteration [1001/1289], Loss: 0.0119 (0.0174), Elapsed Time 595.1343\n",
      "Epoch [27], Iteration [1101/1289], Loss: 0.0064 (0.0174), Elapsed Time 654.0720\n",
      "Epoch [27], Iteration [1201/1289], Loss: 0.0087 (0.0178), Elapsed Time 713.3600\n",
      "Average Loss: 0.017870083451271057\n",
      "Testing 0/10449: Loss 0.15245208144187927\n",
      "Testing 1000/10449: Loss 0.11463708430528641\n",
      "Testing 2000/10449: Loss 0.1141822412610054\n",
      "Testing 3000/10449: Loss 0.11545294523239136\n",
      "Testing 4000/10449: Loss 0.11457570642232895\n",
      "Testing 5000/10449: Loss 0.11357361078262329\n",
      "Testing 6000/10449: Loss 0.1135844960808754\n",
      "Testing 7000/10449: Loss 0.11336668580770493\n",
      "Testing 8000/10449: Loss 0.11437385529279709\n",
      "Testing 9000/10449: Loss 0.11466183513402939\n",
      "Testing 10000/10449: Loss 0.11473368108272552\n",
      "Avg Loss 0.11459434032440186\n",
      "Score 0.6473110868790629\n",
      "Epoch [28], Iteration [1/1289], Loss: 0.0247 (0.0247), Elapsed Time 0.6000\n",
      "Epoch [28], Iteration [101/1289], Loss: 0.0324 (0.0171), Elapsed Time 57.1124\n",
      "Epoch [28], Iteration [201/1289], Loss: 0.0608 (0.0164), Elapsed Time 113.4022\n",
      "Epoch [28], Iteration [301/1289], Loss: 0.0143 (0.0157), Elapsed Time 169.9117\n",
      "Epoch [28], Iteration [401/1289], Loss: 0.0155 (0.0159), Elapsed Time 226.7640\n",
      "Epoch [28], Iteration [501/1289], Loss: 0.0133 (0.0160), Elapsed Time 283.1958\n",
      "Epoch [28], Iteration [601/1289], Loss: 0.0113 (0.0162), Elapsed Time 339.8513\n",
      "Epoch [28], Iteration [701/1289], Loss: 0.0118 (0.0161), Elapsed Time 396.3695\n",
      "Epoch [28], Iteration [801/1289], Loss: 0.0384 (0.0162), Elapsed Time 453.2218\n",
      "Epoch [28], Iteration [901/1289], Loss: 0.0288 (0.0164), Elapsed Time 509.5767\n",
      "Epoch [28], Iteration [1001/1289], Loss: 0.0247 (0.0166), Elapsed Time 566.8148\n",
      "Epoch [28], Iteration [1101/1289], Loss: 0.0250 (0.0167), Elapsed Time 623.3865\n",
      "Epoch [28], Iteration [1201/1289], Loss: 0.0204 (0.0168), Elapsed Time 679.6175\n",
      "Average Loss: 0.016968492418527603\n",
      "Testing 0/10449: Loss 0.09623290598392487\n",
      "Testing 1000/10449: Loss 0.11241869628429413\n",
      "Testing 2000/10449: Loss 0.11013438552618027\n",
      "Testing 3000/10449: Loss 0.11268608272075653\n",
      "Testing 4000/10449: Loss 0.11212752014398575\n",
      "Testing 5000/10449: Loss 0.11209650337696075\n",
      "Testing 6000/10449: Loss 0.11322121322154999\n",
      "Testing 7000/10449: Loss 0.11358533054590225\n",
      "Testing 8000/10449: Loss 0.11458297073841095\n",
      "Testing 9000/10449: Loss 0.11451853811740875\n",
      "Testing 10000/10449: Loss 0.11504089832305908\n",
      "Avg Loss 0.11517463624477386\n",
      "Score 0.6412463316431687\n",
      "Epoch [29], Iteration [1/1289], Loss: 0.0116 (0.0116), Elapsed Time 0.5843\n",
      "Epoch [29], Iteration [101/1289], Loss: 0.0054 (0.0137), Elapsed Time 57.6324\n",
      "Epoch [29], Iteration [201/1289], Loss: 0.0102 (0.0141), Elapsed Time 115.9794\n",
      "Epoch [29], Iteration [301/1289], Loss: 0.0119 (0.0146), Elapsed Time 173.7762\n",
      "Epoch [29], Iteration [401/1289], Loss: 0.0267 (0.0148), Elapsed Time 230.2945\n",
      "Epoch [29], Iteration [501/1289], Loss: 0.0263 (0.0148), Elapsed Time 286.6806\n",
      "Epoch [29], Iteration [601/1289], Loss: 0.0177 (0.0147), Elapsed Time 343.6742\n",
      "Epoch [29], Iteration [701/1289], Loss: 0.0099 (0.0148), Elapsed Time 400.2574\n",
      "Epoch [29], Iteration [801/1289], Loss: 0.0082 (0.0149), Elapsed Time 456.9013\n",
      "Epoch [29], Iteration [901/1289], Loss: 0.0108 (0.0150), Elapsed Time 513.6030\n",
      "Epoch [29], Iteration [1001/1289], Loss: 0.0146 (0.0151), Elapsed Time 570.4153\n",
      "Epoch [29], Iteration [1101/1289], Loss: 0.0104 (0.0153), Elapsed Time 626.9522\n",
      "Epoch [29], Iteration [1201/1289], Loss: 0.0096 (0.0154), Elapsed Time 683.7529\n",
      "Average Loss: 0.015495224855840206\n",
      "Testing 0/10449: Loss 0.16770541667938232\n",
      "Testing 1000/10449: Loss 0.1175125390291214\n",
      "Testing 2000/10449: Loss 0.11331012845039368\n",
      "Testing 3000/10449: Loss 0.11468459665775299\n",
      "Testing 4000/10449: Loss 0.11453860998153687\n",
      "Testing 5000/10449: Loss 0.11455442011356354\n",
      "Testing 6000/10449: Loss 0.11523433029651642\n",
      "Testing 7000/10449: Loss 0.11595652252435684\n",
      "Testing 8000/10449: Loss 0.11732087284326553\n",
      "Testing 9000/10449: Loss 0.11753024905920029\n",
      "Testing 10000/10449: Loss 0.11757310479879379\n",
      "Avg Loss 0.11771939694881439\n",
      "Score 0.6347668258448121\n",
      "Epoch [30], Iteration [1/1289], Loss: 0.0115 (0.0115), Elapsed Time 0.6029\n",
      "Epoch [30], Iteration [101/1289], Loss: 0.0077 (0.0136), Elapsed Time 61.1952\n",
      "Epoch [30], Iteration [201/1289], Loss: 0.0058 (0.0128), Elapsed Time 120.6261\n",
      "Epoch [30], Iteration [301/1289], Loss: 0.0163 (0.0130), Elapsed Time 181.4387\n",
      "Epoch [30], Iteration [401/1289], Loss: 0.0086 (0.0135), Elapsed Time 241.7918\n",
      "Epoch [30], Iteration [501/1289], Loss: 0.0135 (0.0137), Elapsed Time 298.7267\n",
      "Epoch [30], Iteration [601/1289], Loss: 0.0212 (0.0143), Elapsed Time 356.3304\n",
      "Epoch [30], Iteration [701/1289], Loss: 0.0063 (0.0143), Elapsed Time 412.6322\n",
      "Epoch [30], Iteration [801/1289], Loss: 0.0132 (0.0143), Elapsed Time 470.1626\n",
      "Epoch [30], Iteration [901/1289], Loss: 0.0079 (0.0144), Elapsed Time 527.2468\n",
      "Epoch [30], Iteration [1001/1289], Loss: 0.0071 (0.0146), Elapsed Time 583.8494\n",
      "Epoch [30], Iteration [1101/1289], Loss: 0.0114 (0.0145), Elapsed Time 641.2920\n",
      "Epoch [30], Iteration [1201/1289], Loss: 0.0257 (0.0145), Elapsed Time 697.7277\n",
      "Average Loss: 0.014723188243806362\n",
      "Testing 0/10449: Loss 0.10595161467790604\n",
      "Testing 1000/10449: Loss 0.12344150990247726\n",
      "Testing 2000/10449: Loss 0.11898058652877808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 3000/10449: Loss 0.1204676479101181\n",
      "Testing 4000/10449: Loss 0.12003310769796371\n",
      "Testing 5000/10449: Loss 0.11931385844945908\n",
      "Testing 6000/10449: Loss 0.11972671002149582\n",
      "Testing 7000/10449: Loss 0.11975670605897903\n",
      "Testing 8000/10449: Loss 0.12057864665985107\n",
      "Testing 9000/10449: Loss 0.12027401477098465\n",
      "Testing 10000/10449: Loss 0.1203642189502716\n",
      "Avg Loss 0.12031877040863037\n",
      "Score 0.6459856467201924\n",
      "Epoch [31], Iteration [1/1289], Loss: 0.0029 (0.0029), Elapsed Time 0.5836\n",
      "Epoch [31], Iteration [101/1289], Loss: 0.0127 (0.0134), Elapsed Time 59.0557\n",
      "Epoch [31], Iteration [201/1289], Loss: 0.0365 (0.0126), Elapsed Time 116.0851\n",
      "Epoch [31], Iteration [301/1289], Loss: 0.0165 (0.0130), Elapsed Time 173.3989\n",
      "Epoch [31], Iteration [401/1289], Loss: 0.0179 (0.0129), Elapsed Time 230.1573\n",
      "Epoch [31], Iteration [501/1289], Loss: 0.0128 (0.0129), Elapsed Time 287.3784\n",
      "Epoch [31], Iteration [601/1289], Loss: 0.0035 (0.0127), Elapsed Time 344.1567\n",
      "Epoch [31], Iteration [701/1289], Loss: 0.0233 (0.0129), Elapsed Time 401.7673\n",
      "Epoch [31], Iteration [801/1289], Loss: 0.0262 (0.0131), Elapsed Time 460.2697\n",
      "Epoch [31], Iteration [901/1289], Loss: 0.0197 (0.0132), Elapsed Time 518.4922\n",
      "Epoch [31], Iteration [1001/1289], Loss: 0.0117 (0.0133), Elapsed Time 576.2242\n",
      "Epoch [31], Iteration [1101/1289], Loss: 0.0131 (0.0134), Elapsed Time 633.1766\n",
      "Epoch [31], Iteration [1201/1289], Loss: 0.0155 (0.0136), Elapsed Time 690.9556\n",
      "Average Loss: 0.013704420998692513\n",
      "Testing 0/10449: Loss 0.15655341744422913\n",
      "Testing 1000/10449: Loss 0.12121406197547913\n",
      "Testing 2000/10449: Loss 0.11889076232910156\n",
      "Testing 3000/10449: Loss 0.11917553842067719\n",
      "Testing 4000/10449: Loss 0.11850583553314209\n",
      "Testing 5000/10449: Loss 0.11818855255842209\n",
      "Testing 6000/10449: Loss 0.11872414499521255\n",
      "Testing 7000/10449: Loss 0.11858958750963211\n",
      "Testing 8000/10449: Loss 0.12005176395177841\n",
      "Testing 9000/10449: Loss 0.12046339362859726\n",
      "Testing 10000/10449: Loss 0.12071263790130615\n",
      "Avg Loss 0.12054044753313065\n",
      "Score 0.655899316114579\n",
      "Epoch [32], Iteration [1/1289], Loss: 0.0212 (0.0212), Elapsed Time 0.5991\n",
      "Epoch [32], Iteration [101/1289], Loss: 0.0062 (0.0140), Elapsed Time 57.8412\n",
      "Epoch [32], Iteration [201/1289], Loss: 0.0188 (0.0137), Elapsed Time 114.9257\n",
      "Epoch [32], Iteration [301/1289], Loss: 0.0238 (0.0138), Elapsed Time 172.3947\n",
      "Epoch [32], Iteration [401/1289], Loss: 0.0114 (0.0132), Elapsed Time 230.4719\n",
      "Epoch [32], Iteration [501/1289], Loss: 0.0052 (0.0132), Elapsed Time 287.7859\n",
      "Epoch [32], Iteration [601/1289], Loss: 0.0198 (0.0131), Elapsed Time 345.6030\n",
      "Epoch [32], Iteration [701/1289], Loss: 0.0160 (0.0132), Elapsed Time 404.1958\n",
      "Epoch [32], Iteration [801/1289], Loss: 0.0056 (0.0134), Elapsed Time 462.1857\n",
      "Epoch [32], Iteration [901/1289], Loss: 0.0241 (0.0134), Elapsed Time 520.6795\n",
      "Epoch [32], Iteration [1001/1289], Loss: 0.0099 (0.0134), Elapsed Time 579.9563\n",
      "Epoch [32], Iteration [1101/1289], Loss: 0.0098 (0.0135), Elapsed Time 638.1611\n",
      "Epoch [32], Iteration [1201/1289], Loss: 0.0185 (0.0137), Elapsed Time 694.7355\n",
      "Average Loss: 0.013857103884220123\n",
      "Testing 0/10449: Loss 0.08889293670654297\n",
      "Testing 1000/10449: Loss 0.12211015075445175\n",
      "Testing 2000/10449: Loss 0.1201726496219635\n",
      "Testing 3000/10449: Loss 0.1228114441037178\n",
      "Testing 4000/10449: Loss 0.12234778702259064\n",
      "Testing 5000/10449: Loss 0.12128056585788727\n",
      "Testing 6000/10449: Loss 0.12114020437002182\n",
      "Testing 7000/10449: Loss 0.12148244678974152\n",
      "Testing 8000/10449: Loss 0.12200983613729477\n",
      "Testing 9000/10449: Loss 0.12180198729038239\n",
      "Testing 10000/10449: Loss 0.12235690653324127\n",
      "Avg Loss 0.12206307053565979\n",
      "Score 0.6606663517678847\n",
      "Epoch [33], Iteration [1/1289], Loss: 0.0062 (0.0062), Elapsed Time 0.5751\n",
      "Epoch [33], Iteration [101/1289], Loss: 0.0169 (0.0116), Elapsed Time 58.4234\n",
      "Epoch [33], Iteration [201/1289], Loss: 0.0044 (0.0117), Elapsed Time 118.9579\n",
      "Epoch [33], Iteration [301/1289], Loss: 0.0067 (0.0116), Elapsed Time 179.9043\n",
      "Epoch [33], Iteration [401/1289], Loss: 0.0120 (0.0118), Elapsed Time 237.6906\n",
      "Epoch [33], Iteration [501/1289], Loss: 0.0126 (0.0122), Elapsed Time 295.0815\n",
      "Epoch [33], Iteration [601/1289], Loss: 0.0108 (0.0124), Elapsed Time 354.4081\n",
      "Epoch [33], Iteration [701/1289], Loss: 0.0121 (0.0124), Elapsed Time 413.4010\n",
      "Epoch [33], Iteration [801/1289], Loss: 0.0145 (0.0124), Elapsed Time 472.7362\n",
      "Epoch [33], Iteration [901/1289], Loss: 0.0110 (0.0125), Elapsed Time 533.1961\n",
      "Epoch [33], Iteration [1001/1289], Loss: 0.0168 (0.0126), Elapsed Time 592.3018\n",
      "Epoch [33], Iteration [1101/1289], Loss: 0.0115 (0.0129), Elapsed Time 651.5843\n",
      "Epoch [33], Iteration [1201/1289], Loss: 0.0043 (0.0129), Elapsed Time 708.2896\n",
      "Average Loss: 0.012910917401313782\n",
      "Testing 0/10449: Loss 0.04783178120851517\n",
      "Testing 1000/10449: Loss 0.11815565079450607\n",
      "Testing 2000/10449: Loss 0.1190236508846283\n",
      "Testing 3000/10449: Loss 0.12259694188833237\n",
      "Testing 4000/10449: Loss 0.12068319320678711\n",
      "Testing 5000/10449: Loss 0.12039975821971893\n",
      "Testing 6000/10449: Loss 0.12100910395383835\n",
      "Testing 7000/10449: Loss 0.12144502252340317\n",
      "Testing 8000/10449: Loss 0.12287973612546921\n",
      "Testing 9000/10449: Loss 0.1226259097456932\n",
      "Testing 10000/10449: Loss 0.12267032265663147\n",
      "Avg Loss 0.12284987419843674\n",
      "Score 0.6518446454930495\n",
      "Epoch [34], Iteration [1/1289], Loss: 0.0072 (0.0072), Elapsed Time 0.5921\n",
      "Epoch [34], Iteration [101/1289], Loss: 0.0088 (0.0110), Elapsed Time 57.4518\n",
      "Epoch [34], Iteration [201/1289], Loss: 0.0178 (0.0106), Elapsed Time 114.4862\n",
      "Epoch [34], Iteration [301/1289], Loss: 0.0310 (0.0107), Elapsed Time 170.9415\n",
      "Epoch [34], Iteration [401/1289], Loss: 0.0062 (0.0106), Elapsed Time 230.7103\n",
      "Epoch [34], Iteration [501/1289], Loss: 0.0228 (0.0106), Elapsed Time 290.7125\n",
      "Epoch [34], Iteration [601/1289], Loss: 0.0171 (0.0107), Elapsed Time 350.8684\n",
      "Epoch [34], Iteration [701/1289], Loss: 0.0096 (0.0110), Elapsed Time 411.0397\n",
      "Epoch [34], Iteration [801/1289], Loss: 0.0111 (0.0110), Elapsed Time 471.5578\n",
      "Epoch [34], Iteration [901/1289], Loss: 0.0164 (0.0111), Elapsed Time 531.5853\n",
      "Epoch [34], Iteration [1001/1289], Loss: 0.0055 (0.0113), Elapsed Time 591.6344\n",
      "Epoch [34], Iteration [1101/1289], Loss: 0.0307 (0.0115), Elapsed Time 651.4341\n",
      "Epoch [34], Iteration [1201/1289], Loss: 0.0209 (0.0116), Elapsed Time 711.8278\n",
      "Average Loss: 0.011678170412778854\n",
      "Testing 0/10449: Loss 0.09355107694864273\n",
      "Testing 1000/10449: Loss 0.12049530446529388\n",
      "Testing 2000/10449: Loss 0.12222342938184738\n",
      "Testing 3000/10449: Loss 0.12712469696998596\n",
      "Testing 4000/10449: Loss 0.12739069759845734\n",
      "Testing 5000/10449: Loss 0.12793396413326263\n",
      "Testing 6000/10449: Loss 0.12796877324581146\n",
      "Testing 7000/10449: Loss 0.12812146544456482\n",
      "Testing 8000/10449: Loss 0.12845666706562042\n",
      "Testing 9000/10449: Loss 0.12830553948879242\n",
      "Testing 10000/10449: Loss 0.1283058524131775\n",
      "Avg Loss 0.12851539254188538\n",
      "Score 0.647290010256368\n",
      "Epoch [35], Iteration [1/1289], Loss: 0.0293 (0.0293), Elapsed Time 0.6142\n",
      "Epoch [35], Iteration [101/1289], Loss: 0.0081 (0.0112), Elapsed Time 59.4532\n",
      "Epoch [35], Iteration [201/1289], Loss: 0.0179 (0.0105), Elapsed Time 118.3212\n",
      "Epoch [35], Iteration [301/1289], Loss: 0.0085 (0.0104), Elapsed Time 177.5417\n",
      "Epoch [35], Iteration [401/1289], Loss: 0.0070 (0.0104), Elapsed Time 236.5284\n",
      "Epoch [35], Iteration [501/1289], Loss: 0.0200 (0.0106), Elapsed Time 297.5627\n",
      "Epoch [35], Iteration [601/1289], Loss: 0.0077 (0.0107), Elapsed Time 355.0970\n",
      "Epoch [35], Iteration [701/1289], Loss: 0.0109 (0.0108), Elapsed Time 411.6282\n",
      "Epoch [35], Iteration [801/1289], Loss: 0.0305 (0.0109), Elapsed Time 467.8418\n",
      "Epoch [35], Iteration [901/1289], Loss: 0.0103 (0.0109), Elapsed Time 523.4632\n",
      "Epoch [35], Iteration [1001/1289], Loss: 0.0174 (0.0111), Elapsed Time 579.1628\n",
      "Epoch [35], Iteration [1101/1289], Loss: 0.0187 (0.0113), Elapsed Time 634.7135\n",
      "Epoch [35], Iteration [1201/1289], Loss: 0.0071 (0.0114), Elapsed Time 690.6323\n",
      "Average Loss: 0.011561792343854904\n",
      "Testing 0/10449: Loss 0.062143564224243164\n",
      "Testing 1000/10449: Loss 0.1308610588312149\n",
      "Testing 2000/10449: Loss 0.1265653669834137\n",
      "Testing 3000/10449: Loss 0.12586335837841034\n",
      "Testing 4000/10449: Loss 0.12593016028404236\n",
      "Testing 5000/10449: Loss 0.12544570863246918\n",
      "Testing 6000/10449: Loss 0.12497466057538986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 7000/10449: Loss 0.12474077194929123\n",
      "Testing 8000/10449: Loss 0.1260243058204651\n",
      "Testing 9000/10449: Loss 0.12636400759220123\n",
      "Testing 10000/10449: Loss 0.12693214416503906\n",
      "Avg Loss 0.12701071798801422\n",
      "Score 0.6385648444723705\n",
      "Epoch [36], Iteration [1/1289], Loss: 0.0060 (0.0060), Elapsed Time 0.5871\n",
      "Epoch [36], Iteration [101/1289], Loss: 0.0024 (0.0090), Elapsed Time 59.1563\n",
      "Epoch [36], Iteration [201/1289], Loss: 0.0101 (0.0089), Elapsed Time 117.5944\n",
      "Epoch [36], Iteration [301/1289], Loss: 0.0030 (0.0092), Elapsed Time 175.1359\n",
      "Epoch [36], Iteration [401/1289], Loss: 0.0038 (0.0098), Elapsed Time 233.3639\n",
      "Epoch [36], Iteration [501/1289], Loss: 0.0056 (0.0098), Elapsed Time 291.0013\n",
      "Epoch [36], Iteration [601/1289], Loss: 0.0143 (0.0099), Elapsed Time 348.8324\n",
      "Epoch [36], Iteration [701/1289], Loss: 0.0041 (0.0101), Elapsed Time 407.5749\n",
      "Epoch [36], Iteration [801/1289], Loss: 0.0051 (0.0100), Elapsed Time 466.1925\n",
      "Epoch [36], Iteration [901/1289], Loss: 0.0045 (0.0103), Elapsed Time 524.0291\n",
      "Epoch [36], Iteration [1001/1289], Loss: 0.0041 (0.0103), Elapsed Time 581.7429\n",
      "Epoch [36], Iteration [1101/1289], Loss: 0.0016 (0.0103), Elapsed Time 639.4868\n",
      "Epoch [36], Iteration [1201/1289], Loss: 0.0144 (0.0104), Elapsed Time 697.2552\n",
      "Average Loss: 0.010382035747170448\n",
      "Testing 0/10449: Loss 0.04843771830201149\n",
      "Testing 1000/10449: Loss 0.13068124651908875\n",
      "Testing 2000/10449: Loss 0.12701405584812164\n",
      "Testing 3000/10449: Loss 0.1285478174686432\n",
      "Testing 4000/10449: Loss 0.12876494228839874\n",
      "Testing 5000/10449: Loss 0.12692297995090485\n",
      "Testing 6000/10449: Loss 0.1273001879453659\n",
      "Testing 7000/10449: Loss 0.12743110954761505\n",
      "Testing 8000/10449: Loss 0.12838996946811676\n",
      "Testing 9000/10449: Loss 0.1281438171863556\n",
      "Testing 10000/10449: Loss 0.12886928021907806\n",
      "Avg Loss 0.12872980535030365\n",
      "Score 0.64595585509425\n",
      "Epoch [37], Iteration [1/1289], Loss: 0.0120 (0.0120), Elapsed Time 0.5941\n",
      "Epoch [37], Iteration [101/1289], Loss: 0.0040 (0.0086), Elapsed Time 58.0003\n",
      "Epoch [37], Iteration [201/1289], Loss: 0.0113 (0.0093), Elapsed Time 114.4507\n",
      "Epoch [37], Iteration [301/1289], Loss: 0.0065 (0.0098), Elapsed Time 171.0266\n",
      "Epoch [37], Iteration [401/1289], Loss: 0.0158 (0.0100), Elapsed Time 227.7467\n",
      "Epoch [37], Iteration [501/1289], Loss: 0.0142 (0.0102), Elapsed Time 284.1757\n",
      "Epoch [37], Iteration [601/1289], Loss: 0.0175 (0.0100), Elapsed Time 341.0224\n",
      "Epoch [37], Iteration [701/1289], Loss: 0.0064 (0.0100), Elapsed Time 397.4752\n",
      "Epoch [37], Iteration [801/1289], Loss: 0.0146 (0.0100), Elapsed Time 454.3297\n",
      "Epoch [37], Iteration [901/1289], Loss: 0.0218 (0.0100), Elapsed Time 510.9533\n",
      "Epoch [37], Iteration [1001/1289], Loss: 0.0110 (0.0101), Elapsed Time 567.6053\n",
      "Epoch [37], Iteration [1101/1289], Loss: 0.0097 (0.0103), Elapsed Time 625.3470\n",
      "Epoch [37], Iteration [1201/1289], Loss: 0.0105 (0.0104), Elapsed Time 683.0906\n",
      "Average Loss: 0.010546826757490635\n",
      "Testing 0/10449: Loss 0.08943068981170654\n",
      "Testing 1000/10449: Loss 0.12300767749547958\n",
      "Testing 2000/10449: Loss 0.12632934749126434\n",
      "Testing 3000/10449: Loss 0.12749584019184113\n",
      "Testing 4000/10449: Loss 0.12712401151657104\n",
      "Testing 5000/10449: Loss 0.12863698601722717\n",
      "Testing 6000/10449: Loss 0.12825718522071838\n",
      "Testing 7000/10449: Loss 0.12905097007751465\n",
      "Testing 8000/10449: Loss 0.13043421506881714\n",
      "Testing 9000/10449: Loss 0.13074995577335358\n",
      "Testing 10000/10449: Loss 0.1308332085609436\n",
      "Avg Loss 0.1307726353406906\n",
      "Score 0.6556417090546838\n",
      "Epoch [38], Iteration [1/1289], Loss: 0.0085 (0.0085), Elapsed Time 0.5735\n",
      "Epoch [38], Iteration [101/1289], Loss: 0.0262 (0.0094), Elapsed Time 56.0534\n",
      "Epoch [38], Iteration [201/1289], Loss: 0.0068 (0.0099), Elapsed Time 111.5338\n",
      "Epoch [38], Iteration [301/1289], Loss: 0.0111 (0.0096), Elapsed Time 167.3737\n",
      "Epoch [38], Iteration [401/1289], Loss: 0.0047 (0.0091), Elapsed Time 225.0597\n",
      "Epoch [38], Iteration [501/1289], Loss: 0.0046 (0.0094), Elapsed Time 283.1936\n",
      "Epoch [38], Iteration [601/1289], Loss: 0.0075 (0.0095), Elapsed Time 341.7108\n",
      "Epoch [38], Iteration [701/1289], Loss: 0.0099 (0.0098), Elapsed Time 400.0488\n",
      "Epoch [38], Iteration [801/1289], Loss: 0.0045 (0.0101), Elapsed Time 459.6168\n",
      "Epoch [38], Iteration [901/1289], Loss: 0.0045 (0.0101), Elapsed Time 521.3621\n",
      "Epoch [38], Iteration [1001/1289], Loss: 0.0072 (0.0102), Elapsed Time 580.7838\n",
      "Epoch [38], Iteration [1101/1289], Loss: 0.0056 (0.0102), Elapsed Time 638.7612\n",
      "Epoch [38], Iteration [1201/1289], Loss: 0.0130 (0.0102), Elapsed Time 699.4360\n",
      "Average Loss: 0.01035765279084444\n",
      "Testing 0/10449: Loss 0.08673868328332901\n",
      "Testing 1000/10449: Loss 0.1375075727701187\n",
      "Testing 2000/10449: Loss 0.1289294809103012\n",
      "Testing 3000/10449: Loss 0.13032053411006927\n",
      "Testing 4000/10449: Loss 0.13060671091079712\n",
      "Testing 5000/10449: Loss 0.1315111666917801\n",
      "Testing 6000/10449: Loss 0.1312810182571411\n",
      "Testing 7000/10449: Loss 0.1318666636943817\n",
      "Testing 8000/10449: Loss 0.13350434601306915\n",
      "Testing 9000/10449: Loss 0.13388609886169434\n",
      "Testing 10000/10449: Loss 0.1340232640504837\n",
      "Avg Loss 0.13383691012859344\n",
      "Score 0.6550951729107775\n",
      "Epoch [39], Iteration [1/1289], Loss: 0.0053 (0.0053), Elapsed Time 0.5702\n",
      "Epoch [39], Iteration [101/1289], Loss: 0.0022 (0.0085), Elapsed Time 57.9631\n",
      "Epoch [39], Iteration [201/1289], Loss: 0.0052 (0.0093), Elapsed Time 113.4749\n",
      "Epoch [39], Iteration [301/1289], Loss: 0.0071 (0.0099), Elapsed Time 169.8587\n",
      "Epoch [39], Iteration [401/1289], Loss: 0.0117 (0.0097), Elapsed Time 227.3520\n",
      "Epoch [39], Iteration [501/1289], Loss: 0.0023 (0.0095), Elapsed Time 284.3620\n",
      "Epoch [39], Iteration [601/1289], Loss: 0.0158 (0.0094), Elapsed Time 343.0383\n",
      "Epoch [39], Iteration [701/1289], Loss: 0.0021 (0.0093), Elapsed Time 400.9097\n",
      "Epoch [39], Iteration [801/1289], Loss: 0.0136 (0.0092), Elapsed Time 459.1488\n",
      "Epoch [39], Iteration [901/1289], Loss: 0.0169 (0.0093), Elapsed Time 517.9847\n",
      "Epoch [39], Iteration [1001/1289], Loss: 0.0048 (0.0094), Elapsed Time 576.2563\n",
      "Epoch [39], Iteration [1101/1289], Loss: 0.0164 (0.0094), Elapsed Time 635.1073\n",
      "Epoch [39], Iteration [1201/1289], Loss: 0.0041 (0.0095), Elapsed Time 693.2513\n",
      "Average Loss: 0.009576760232448578\n",
      "Testing 0/10449: Loss 0.07315297424793243\n",
      "Testing 1000/10449: Loss 0.12402528524398804\n",
      "Testing 2000/10449: Loss 0.1206229105591774\n",
      "Testing 3000/10449: Loss 0.12488504499197006\n",
      "Testing 4000/10449: Loss 0.1258237659931183\n",
      "Testing 5000/10449: Loss 0.12747414410114288\n",
      "Testing 6000/10449: Loss 0.127456933259964\n",
      "Testing 7000/10449: Loss 0.12877137959003448\n",
      "Testing 8000/10449: Loss 0.1298636496067047\n",
      "Testing 9000/10449: Loss 0.13034500181674957\n",
      "Testing 10000/10449: Loss 0.13125528395175934\n",
      "Avg Loss 0.13140249252319336\n",
      "Score 0.655771362131204\n",
      "Epoch [40], Iteration [1/1289], Loss: 0.0040 (0.0040), Elapsed Time 0.5932\n",
      "Epoch [40], Iteration [101/1289], Loss: 0.0016 (0.0082), Elapsed Time 57.2631\n",
      "Epoch [40], Iteration [201/1289], Loss: 0.0073 (0.0082), Elapsed Time 114.5027\n",
      "Epoch [40], Iteration [301/1289], Loss: 0.0051 (0.0080), Elapsed Time 171.4108\n",
      "Epoch [40], Iteration [401/1289], Loss: 0.0034 (0.0080), Elapsed Time 229.9430\n",
      "Epoch [40], Iteration [501/1289], Loss: 0.0067 (0.0082), Elapsed Time 287.3423\n",
      "Epoch [40], Iteration [601/1289], Loss: 0.0084 (0.0082), Elapsed Time 345.7026\n",
      "Epoch [40], Iteration [701/1289], Loss: 0.0049 (0.0084), Elapsed Time 402.7874\n",
      "Epoch [40], Iteration [801/1289], Loss: 0.0058 (0.0086), Elapsed Time 459.8150\n",
      "Epoch [40], Iteration [901/1289], Loss: 0.0064 (0.0087), Elapsed Time 516.8156\n",
      "Epoch [40], Iteration [1001/1289], Loss: 0.0082 (0.0088), Elapsed Time 573.7574\n",
      "Epoch [40], Iteration [1101/1289], Loss: 0.0091 (0.0089), Elapsed Time 631.2246\n",
      "Epoch [40], Iteration [1201/1289], Loss: 0.0048 (0.0089), Elapsed Time 688.4105\n",
      "Average Loss: 0.009009858593344688\n",
      "Testing 0/10449: Loss 0.11333852261304855\n",
      "Testing 1000/10449: Loss 0.13904686272144318\n",
      "Testing 2000/10449: Loss 0.13654819130897522\n",
      "Testing 3000/10449: Loss 0.13746891915798187\n",
      "Testing 4000/10449: Loss 0.13495011627674103\n",
      "Testing 5000/10449: Loss 0.13462898135185242\n",
      "Testing 6000/10449: Loss 0.13431809842586517\n",
      "Testing 7000/10449: Loss 0.13439464569091797\n",
      "Testing 8000/10449: Loss 0.13523319363594055\n",
      "Testing 9000/10449: Loss 0.13518743216991425\n",
      "Testing 10000/10449: Loss 0.1355171948671341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Loss 0.13534699380397797\n",
      "Score 0.6439986099666418\n",
      "Epoch [41], Iteration [1/1289], Loss: 0.0027 (0.0027), Elapsed Time 0.5744\n",
      "Epoch [41], Iteration [101/1289], Loss: 0.0071 (0.0079), Elapsed Time 57.5706\n",
      "Epoch [41], Iteration [201/1289], Loss: 0.0035 (0.0084), Elapsed Time 114.3131\n",
      "Epoch [41], Iteration [301/1289], Loss: 0.0051 (0.0081), Elapsed Time 171.0314\n",
      "Epoch [41], Iteration [401/1289], Loss: 0.0048 (0.0080), Elapsed Time 227.8947\n",
      "Epoch [41], Iteration [501/1289], Loss: 0.0095 (0.0083), Elapsed Time 284.8739\n",
      "Epoch [41], Iteration [601/1289], Loss: 0.0220 (0.0085), Elapsed Time 341.6446\n",
      "Epoch [41], Iteration [701/1289], Loss: 0.0081 (0.0085), Elapsed Time 398.4111\n",
      "Epoch [41], Iteration [801/1289], Loss: 0.0024 (0.0086), Elapsed Time 454.6770\n",
      "Epoch [41], Iteration [901/1289], Loss: 0.0049 (0.0088), Elapsed Time 511.0400\n",
      "Epoch [41], Iteration [1001/1289], Loss: 0.0056 (0.0089), Elapsed Time 567.3522\n",
      "Epoch [41], Iteration [1101/1289], Loss: 0.0147 (0.0088), Elapsed Time 624.9997\n",
      "Epoch [41], Iteration [1201/1289], Loss: 0.0072 (0.0089), Elapsed Time 683.2424\n",
      "Average Loss: 0.008942130953073502\n",
      "Testing 0/10449: Loss 0.1352892965078354\n",
      "Testing 1000/10449: Loss 0.13203895092010498\n",
      "Testing 2000/10449: Loss 0.1320636123418808\n",
      "Testing 3000/10449: Loss 0.13357225060462952\n",
      "Testing 4000/10449: Loss 0.13349178433418274\n",
      "Testing 5000/10449: Loss 0.13472731411457062\n",
      "Testing 6000/10449: Loss 0.134670689702034\n",
      "Testing 7000/10449: Loss 0.13553138077259064\n",
      "Testing 8000/10449: Loss 0.136611670255661\n",
      "Testing 9000/10449: Loss 0.13664129376411438\n",
      "Testing 10000/10449: Loss 0.13677114248275757\n",
      "Avg Loss 0.13734926283359528\n",
      "Score 0.6345997748786706\n",
      "Epoch [42], Iteration [1/1289], Loss: 0.0020 (0.0020), Elapsed Time 0.5986\n",
      "Epoch [42], Iteration [101/1289], Loss: 0.0066 (0.0082), Elapsed Time 58.6030\n",
      "Epoch [42], Iteration [201/1289], Loss: 0.0064 (0.0074), Elapsed Time 116.2294\n",
      "Epoch [42], Iteration [301/1289], Loss: 0.0116 (0.0079), Elapsed Time 173.8204\n",
      "Epoch [42], Iteration [401/1289], Loss: 0.0047 (0.0081), Elapsed Time 231.9655\n",
      "Epoch [42], Iteration [501/1289], Loss: 0.0081 (0.0084), Elapsed Time 291.0582\n",
      "Epoch [42], Iteration [601/1289], Loss: 0.0054 (0.0084), Elapsed Time 350.8842\n",
      "Epoch [42], Iteration [701/1289], Loss: 0.0033 (0.0083), Elapsed Time 411.2535\n",
      "Epoch [42], Iteration [801/1289], Loss: 0.0013 (0.0084), Elapsed Time 470.8327\n",
      "Epoch [42], Iteration [901/1289], Loss: 0.0043 (0.0086), Elapsed Time 529.7278\n",
      "Epoch [42], Iteration [1001/1289], Loss: 0.0099 (0.0086), Elapsed Time 589.4684\n",
      "Epoch [42], Iteration [1101/1289], Loss: 0.0033 (0.0086), Elapsed Time 648.7653\n",
      "Epoch [42], Iteration [1201/1289], Loss: 0.0027 (0.0086), Elapsed Time 708.0559\n",
      "Average Loss: 0.008681511506438255\n",
      "Testing 0/10449: Loss 0.36092671751976013\n",
      "Testing 1000/10449: Loss 0.13182783126831055\n",
      "Testing 2000/10449: Loss 0.13418301939964294\n",
      "Testing 3000/10449: Loss 0.13393376767635345\n",
      "Testing 4000/10449: Loss 0.13500405848026276\n",
      "Testing 5000/10449: Loss 0.1357884258031845\n",
      "Testing 6000/10449: Loss 0.1354774385690689\n",
      "Testing 7000/10449: Loss 0.13644428551197052\n",
      "Testing 8000/10449: Loss 0.1373249888420105\n",
      "Testing 9000/10449: Loss 0.13706821203231812\n",
      "Testing 10000/10449: Loss 0.13748601078987122\n",
      "Avg Loss 0.13740850985050201\n",
      "Score 0.650412461574725\n",
      "Epoch [43], Iteration [1/1289], Loss: 0.0045 (0.0045), Elapsed Time 0.5869\n",
      "Epoch [43], Iteration [101/1289], Loss: 0.0051 (0.0084), Elapsed Time 59.7343\n",
      "Epoch [43], Iteration [201/1289], Loss: 0.0082 (0.0080), Elapsed Time 118.6081\n",
      "Epoch [43], Iteration [301/1289], Loss: 0.0076 (0.0080), Elapsed Time 177.6859\n",
      "Epoch [43], Iteration [401/1289], Loss: 0.0131 (0.0080), Elapsed Time 235.8988\n",
      "Epoch [43], Iteration [501/1289], Loss: 0.0041 (0.0084), Elapsed Time 295.1205\n",
      "Epoch [43], Iteration [601/1289], Loss: 0.0020 (0.0084), Elapsed Time 355.6725\n",
      "Epoch [43], Iteration [701/1289], Loss: 0.0046 (0.0086), Elapsed Time 416.4253\n",
      "Epoch [43], Iteration [801/1289], Loss: 0.0024 (0.0087), Elapsed Time 476.0928\n",
      "Epoch [43], Iteration [901/1289], Loss: 0.0076 (0.0087), Elapsed Time 535.8452\n",
      "Epoch [43], Iteration [1001/1289], Loss: 0.0039 (0.0087), Elapsed Time 594.8793\n",
      "Epoch [43], Iteration [1101/1289], Loss: 0.0092 (0.0086), Elapsed Time 654.1389\n",
      "Epoch [43], Iteration [1201/1289], Loss: 0.0017 (0.0087), Elapsed Time 713.3256\n",
      "Average Loss: 0.008650925941765308\n",
      "Testing 0/10449: Loss 0.02138896845281124\n",
      "Testing 1000/10449: Loss 0.13080695271492004\n",
      "Testing 2000/10449: Loss 0.1321035623550415\n",
      "Testing 3000/10449: Loss 0.1346179097890854\n",
      "Testing 4000/10449: Loss 0.1345357596874237\n",
      "Testing 5000/10449: Loss 0.13414780795574188\n",
      "Testing 6000/10449: Loss 0.13447442650794983\n",
      "Testing 7000/10449: Loss 0.13540315628051758\n",
      "Testing 8000/10449: Loss 0.1372734159231186\n",
      "Testing 9000/10449: Loss 0.13668498396873474\n",
      "Testing 10000/10449: Loss 0.13706250488758087\n",
      "Avg Loss 0.13745710253715515\n",
      "Score 0.6526467994728653\n",
      "Epoch [44], Iteration [1/1289], Loss: 0.0048 (0.0048), Elapsed Time 0.5785\n",
      "Epoch [44], Iteration [101/1289], Loss: 0.0017 (0.0078), Elapsed Time 57.6709\n",
      "Epoch [44], Iteration [201/1289], Loss: 0.0049 (0.0070), Elapsed Time 114.4243\n",
      "Epoch [44], Iteration [301/1289], Loss: 0.0038 (0.0067), Elapsed Time 172.1498\n",
      "Epoch [44], Iteration [401/1289], Loss: 0.0105 (0.0071), Elapsed Time 229.1377\n",
      "Epoch [44], Iteration [501/1289], Loss: 0.0081 (0.0072), Elapsed Time 287.5606\n",
      "Epoch [44], Iteration [601/1289], Loss: 0.0050 (0.0075), Elapsed Time 345.1574\n",
      "Epoch [44], Iteration [701/1289], Loss: 0.0082 (0.0074), Elapsed Time 403.2787\n",
      "Epoch [44], Iteration [801/1289], Loss: 0.0159 (0.0076), Elapsed Time 460.9032\n",
      "Epoch [44], Iteration [901/1289], Loss: 0.0029 (0.0078), Elapsed Time 518.4259\n",
      "Epoch [44], Iteration [1001/1289], Loss: 0.0088 (0.0079), Elapsed Time 575.4905\n"
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_preds, thresh=0.5, eps=1e-20):\n",
    "    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n",
    "    truepos = preds_bin * y_true\n",
    "    \n",
    "    p = truepos.sum() / (preds_bin.sum() + eps) # take sums and calculate precision on scalars\n",
    "    r = truepos.sum() / (y_true.sum() + eps) # take sums and calculate recall on scalars\n",
    "    \n",
    "    f1 = 2*p*r / (p+r+eps) # we calculate f1 on scalars\n",
    "    return f1\n",
    "\n",
    "def f1_macro(y_true, y_preds, thresh=0.5, eps=1e-20):\n",
    "    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n",
    "    truepos = preds_bin * y_true\n",
    "\n",
    "    p = truepos.sum(axis=0) / (preds_bin.sum(axis=0) + eps) # sum along axis=0 (classes)\n",
    "                                                            # and calculate precision array\n",
    "    r = truepos.sum(axis=0) / (y_true.sum(axis=0) + eps)    # sum along axis=0 (classes) \n",
    "                                                            #  and calculate recall array\n",
    "\n",
    "    f1 = 2*p*r / (p+r+eps) # we calculate f1 on arrays\n",
    "    return np.mean(f1)\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = torch.nn.functional.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "def train_and_val(model_name, split, batch_size, epochs, lr, start_epoch):\n",
    "    \n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes = 1000)\n",
    "    model = construct_rgby_model(model)\n",
    "            \n",
    "    num_features = model.last_linear.in_features \n",
    "    model.last_linear = torch.nn.Linear(num_features, 28)\n",
    "    \n",
    "    if glob.glob('{}_rgby_0*'.format(model_name)):\n",
    "        pth_file = torch.load('{}_rgby_0.pth.tar'.format(model_name))\n",
    "        state_dict = pth_file['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        start_epoch = pth_file['epoch']\n",
    "        \n",
    "    model.cuda()\n",
    "\n",
    "    train_dataset = AtlasData(split = split, train = True, model = model_name)\n",
    "    val_dataset = AtlasData(split = split, train = False, model = model_name)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
    "    \n",
    "    log_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    focal_loss = FocalLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)   \n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch,epochs+1):\n",
    "        \n",
    "        train(model, train_loader, optimizer, log_loss, focal_loss, epoch)\n",
    "        avg_loss, f1_score = validate(model, val_loader, log_loss, focal_loss, epoch)\n",
    "                    \n",
    "        if epoch % 10 == 0:\n",
    "            filename = '{}_rgby_{}_{}.pth.tar'.format(model_name, split, epoch)\n",
    "        else:\n",
    "            filename = '{}_rgby_{}.pth.tar'.format(model_name, split)\n",
    "            \n",
    "        state = {'loss': avg_loss, 'f1_score': f1_score, 'epoch': epoch+1, 'state_dict': model.state_dict()}           \n",
    "        torch.save(state, filename)\n",
    "\n",
    "    \n",
    "def train(model, train_loader, optimizer, log_loss, focal_loss, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i, (images, label_arrs, labels) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        label_arrs = label_arrs.cuda()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = log_loss(outputs, label_arrs)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"Epoch [{}], Iteration [{}/{}], Loss: {:.4f} ({:.4f}), Elapsed Time {:.4f}\"\n",
    "                .format(epoch, i+1, len(train_loader), loss.data[0], sum(losses)/len(losses), elapsed))\n",
    "            \n",
    "    print(\"Average Loss: {}\".format(sum(losses)/len(losses)))\n",
    "            \n",
    "\n",
    "def validate(model, val_loader, log_loss, focal_loss, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    y_pred = np.zeros(len(val_loader) * 28).reshape(len(val_loader), 28)\n",
    "    y_true = np.zeros(len(val_loader) * 28).reshape(len(val_loader), 28)\n",
    "\n",
    "    for i, (images, label_arrs, labels) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "        label_arrs_cuda = label_arrs.cuda()\n",
    "\n",
    "        raw_predictions = model(images)\n",
    "        outputs = raw_predictions.data\n",
    "        \n",
    "        loss = log_loss(outputs, label_arrs_cuda) \n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        predictions = np.arange(28)[raw_predictions.data[0] > 0.15]\n",
    "        \n",
    "        y_pred[i,:] = predictions\n",
    "        y_true[i,:] = label_arrs\n",
    "        \n",
    "        if sum(predictions) == 0:\n",
    "            prediction = np.argmax(raw_predictions.detach().cpu().numpy())\n",
    "            predictions = np.zeros(28)\n",
    "            np.put(predictions, prediction, 1)\n",
    "        \n",
    "        \n",
    "        if i%1000==0:\n",
    "            print('Testing {}/{}: Loss {}'.format(i, \n",
    "                                                 len(val_loader), \n",
    "                                                 sum(losses)/len(losses)))\n",
    "                                                                         \n",
    "    score = f1_macro(y_true, y_pred)\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    print(\"Avg Loss {}\".format(avg_loss))\n",
    "    print(\"Score {}\".format(score))\n",
    "\n",
    "    return avg_loss, score\n",
    "\n",
    "split = 0\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "\n",
    "model_list = ['xception',\n",
    "              'nasnetamobile',\n",
    "              'resnet34', \n",
    "              'senet154',\n",
    "              'polynet',]\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(\"Training {}\".format(model_name))\n",
    "    train_and_val(model_name, split, batch_size, epochs, lr, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pretrainedmodels.__dict__['polynet']()\n",
    "\n",
    "print(model.mean)\n",
    "print(model.std)\n",
    "print(model.input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalAtlasData(Dataset):\n",
    "    def __init__(self, model = 'bninception'):\n",
    "        self.image_ids = sorted(set([x.split('_')[0] for x in os.listdir('data/test')]))\n",
    "        \n",
    "        self.input_size = model_configs[model]['input_size']\n",
    "        self.input_mean = model_configs[model]['input_mean']\n",
    "        self.input_std = model_configs[model]['input_std']\n",
    "        \n",
    "        self.transforms = transforms.Compose([transforms.Resize(self.input_size),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(self.input_mean, self.input_std),\n",
    "                                            ])\n",
    "        \n",
    "        \n",
    "    def load_image_stack(self, image_id):\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "        absolute_paths = [\"data/test/{}_{}.png\".format(image_id, color) for color in colors]\n",
    "        \n",
    "        images = [skimage.io.imread(path) for path in absolute_paths]\n",
    "        \n",
    "        image_red = images[0]\n",
    "        image_green = images[1] + (images[3]/2).astype(np.uint8)\n",
    "        image_blue = images[2] + (images[3]/2).astype(np.uint8)\n",
    "        \n",
    "        final_image = np.stack((image_red, image_green, image_blue), -1)\n",
    "        to_display = Image.fromarray(final_image)\n",
    "        return to_display\n",
    "    \n",
    "    def dump_image(self, i):\n",
    "        image = self.load_image_stack(self.image_ids[i])\n",
    "        image_name = \"data/test/{}_{}.png\".format(self.image_ids[i], 'stacked')\n",
    "        image.save(image_name)\n",
    "        print(\"Saved\", image_name)\n",
    "        \n",
    "    def load_image(self, i):\n",
    "        image_id = self.image_ids[i]\n",
    "        image_path = \"data/test/{}_{}.png\".format(image_id, 'stacked')\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        return image, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image, image_id = self.load_image(i)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        return image_id, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "from torch.nn import Softmax\n",
    "\n",
    "def generate_preds(model_name):\n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes = 1000, pretrained = 'imagenet')\n",
    "    in_features = model.last_linear.in_features\n",
    "    model.last_linear = torch.nn.Linear(in_features, 28)\n",
    "    \n",
    "    if model_name == 'polynet':\n",
    "        model = torch.nn.DataParallel(model, device_ids = [0,1,2,3]).cuda()\n",
    "        model.load_state_dict(torch.load('{}_0.pth.tar'.format(model_name)))\n",
    "        model = model.eval()\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('{}_0.pth.tar'.format(model_name)))\n",
    "        model = model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "    eval_data = EvalAtlasData(model = model_name)\n",
    "    dataloader = DataLoader(eval_data, 1, False)\n",
    "    \n",
    "    preds = []\n",
    "    for i, (image_id, images) in enumerate(dataloader):\n",
    "        images = images.cuda()\n",
    "\n",
    "        raw_predictions = (model(images))\n",
    "        predictions = np.argwhere(raw_predictions.data[0] > 0.15)\n",
    "        try:\n",
    "            num_predictions = len(predictions.data[0])\n",
    "        except IndexError:\n",
    "            num_predictions = 0\n",
    "\n",
    "        print('-----------------------------------------------------')\n",
    "        print(image_id[0])\n",
    "        print('Raw Prediction', raw_predictions)\n",
    "        if num_predictions == 0:\n",
    "            print('No value passed the threshold')\n",
    "            predictions = [np.argmax(raw_predictions.detach().cpu().numpy())]\n",
    "            num_predictions = 1\n",
    "            print(\"Prediction:\", predictions)\n",
    "            print(\"Number of predictions\", num_predictions)\n",
    "        else:\n",
    "            predictions = predictions.data[0].tolist()\n",
    "            print(\"Prediction:\", predictions)\n",
    "            print(\"Number of predictions\", num_predictions)\n",
    "\n",
    "        predicted = ' '.join('%d' % prediction for prediction in predictions)\n",
    "        print(image_id[0])\n",
    "        print(predicted)\n",
    "        pred = dict(Id = image_id[0], Predicted = predicted)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    df = pd.DataFrame(preds)\n",
    "    df.to_csv('{}.csv'.format(model_name), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. Use Focal Loss https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-fast-ai\n",
    "2. Somehow use the Y channel\n",
    "\n",
    "Trained:\n",
    "1. InceptionV4 100 epochs\n",
    "2. SE-ResNext 33 epochs\n",
    "3. PolyNet 33 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG:\n\tUnexpected key(s) in state_dict: \"_features.0.weight\", \"_features.0.bias\", \"_features.1.weight\", \"_features.1.bias\", \"_features.1.running_mean\", \"_features.1.running_var\", \"_features.3.weight\", \"_features.3.bias\", \"_features.4.weight\", \"_features.4.bias\", \"_features.4.running_mean\", \"_features.4.running_var\", \"_features.7.weight\", \"_features.7.bias\", \"_features.8.weight\", \"_features.8.bias\", \"_features.8.running_mean\", \"_features.8.running_var\", \"_features.10.weight\", \"_features.10.bias\", \"_features.11.weight\", \"_features.11.bias\", \"_features.11.running_mean\", \"_features.11.running_var\", \"_features.14.weight\", \"_features.14.bias\", \"_features.15.weight\", \"_features.15.bias\", \"_features.15.running_mean\", \"_features.15.running_var\", \"_features.17.weight\", \"_features.17.bias\", \"_features.18.weight\", \"_features.18.bias\", \"_features.18.running_mean\", \"_features.18.running_var\", \"_features.20.weight\", \"_features.20.bias\", \"_features.21.weight\", \"_features.21.bias\", \"_features.21.running_mean\", \"_features.21.running_var\", \"_features.23.weight\", \"_features.23.bias\", \"_features.24.weight\", \"_features.24.bias\", \"_features.24.running_mean\", \"_features.24.running_var\", \"_features.27.weight\", \"_features.27.bias\", \"_features.28.weight\", \"_features.28.bias\", \"_features.28.running_mean\", \"_features.28.running_var\", \"_features.30.weight\", \"_features.30.bias\", \"_features.31.weight\", \"_features.31.bias\", \"_features.31.running_mean\", \"_features.31.running_var\", \"_features.33.weight\", \"_features.33.bias\", \"_features.34.weight\", \"_features.34.bias\", \"_features.34.running_mean\", \"_features.34.running_var\", \"_features.36.weight\", \"_features.36.bias\", \"_features.37.weight\", \"_features.37.bias\", \"_features.37.running_mean\", \"_features.37.running_var\", \"_features.40.weight\", \"_features.40.bias\", \"_features.41.weight\", \"_features.41.bias\", \"_features.41.running_mean\", \"_features.41.running_var\", \"_features.43.weight\", \"_features.43.bias\", \"_features.44.weight\", \"_features.44.bias\", \"_features.44.running_mean\", \"_features.44.running_var\", \"_features.46.weight\", \"_features.46.bias\", \"_features.47.weight\", \"_features.47.bias\", \"_features.47.running_mean\", \"_features.47.running_var\", \"_features.49.weight\", \"_features.49.bias\", \"_features.50.weight\", \"_features.50.bias\", \"_features.50.running_mean\", \"_features.50.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75f7c9ca9e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg19_bn_0_40.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pytorch_new/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tUnexpected key(s) in state_dict: \"_features.0.weight\", \"_features.0.bias\", \"_features.1.weight\", \"_features.1.bias\", \"_features.1.running_mean\", \"_features.1.running_var\", \"_features.3.weight\", \"_features.3.bias\", \"_features.4.weight\", \"_features.4.bias\", \"_features.4.running_mean\", \"_features.4.running_var\", \"_features.7.weight\", \"_features.7.bias\", \"_features.8.weight\", \"_features.8.bias\", \"_features.8.running_mean\", \"_features.8.running_var\", \"_features.10.weight\", \"_features.10.bias\", \"_features.11.weight\", \"_features.11.bias\", \"_features.11.running_mean\", \"_features.11.running_var\", \"_features.14.weight\", \"_features.14.bias\", \"_features.15.weight\", \"_features.15.bias\", \"_features.15.running_mean\", \"_features.15.running_var\", \"_features.17.weight\", \"_features.17.bias\", \"_features.18.weight\", \"_features.18.bias\", \"_features.18.running_mean\", \"_features.18.running_var\", \"_features.20.weight\", \"_features.20.bias\", \"_features.21.weight\", \"_features.21.bias\", \"_features.21.running_mean\", \"_features.21.running_var\", \"_features.23.weight\", \"_features.23.bias\", \"_features.24.weight\", \"_features.24.bias\", \"_features.24.running_mean\", \"_features.24.running_var\", \"_features.27.weight\", \"_features.27.bias\", \"_features.28.weight\", \"_features.28.bias\", \"_features.28.running_mean\", \"_features.28.running_var\", \"_features.30.weight\", \"_features.30.bias\", \"_features.31.weight\", \"_features.31.bias\", \"_features.31.running_mean\", \"_features.31.running_var\", \"_features.33.weight\", \"_features.33.bias\", \"_features.34.weight\", \"_features.34.bias\", \"_features.34.running_mean\", \"_features.34.running_var\", \"_features.36.weight\", \"_features.36.bias\", \"_features.37.weight\", \"_features.37.bias\", \"_features.37.running_mean\", \"_features.37.running_var\", \"_features.40.weight\", \"_features.40.bias\", \"_features.41.weight\", \"_features.41.bias\", \"_features.41.running_mean\", \"_features.41.running_var\", \"_features.43.weight\", \"_features.43.bias\", \"_features.44.weight\", \"_features.44.bias\", \"_features.44.running_mean\", \"_features.44.running_var\", \"_features.46.weight\", \"_features.46.bias\", \"_features.47.weight\", \"_features.47.bias\", \"_features.47.running_mean\", \"_features.47.running_var\", \"_features.49.weight\", \"_features.49.bias\", \"_features.50.weight\", \"_features.50.bias\", \"_features.50.running_mean\", \"_features.50.running_var\". "
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "import torch\n",
    "\n",
    "model = pretrainedmodels.__dict__['vgg19_bn']()\n",
    "in_features = model.last_linear.in_features\n",
    "\n",
    "model.last_linear = torch.nn.Linear(in_features, 28)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('vgg19_bn_0_40.pth.tar')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
