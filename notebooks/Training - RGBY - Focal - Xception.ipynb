{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pretrainedmodels\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from transforms import *\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Channel Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {'polynet':{\n",
    "                   'input_size': 331,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'resnet34':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'senet154':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.485, 0.456, 0.406, 0.406],\n",
    "                   'input_std' : [0.229, 0.224, 0.225, 0.225]\n",
    "                    },\n",
    "                 'nasnetamobile':{\n",
    "                   'input_size': 224,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    },\n",
    "                 'bninception':{\n",
    "                   'input_size': 299,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    },\n",
    "                 'xception':{\n",
    "                   'input_size': 299,\n",
    "                   'input_mean': [0.5],\n",
    "                   'input_std' : [0.5]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "class AtlasData(Dataset):\n",
    "    def __init__(self, split, train = True, model = 'bninception'):\n",
    "        self.split = split\n",
    "        self.train = train\n",
    "        self.train_str = 'train' if self.train else 'test'\n",
    "        self.text_file = 'data/atlas_{}_split_{}.txt'.format(self.train_str, self.split)\n",
    "        \n",
    "        self.data = [[y for y in x.strip().split(' ')] for x in open(self.text_file, 'r').readlines()]\n",
    "        self.imgs = [x[0] for x in self.data]\n",
    "        self.labels = [[int(p) for p in x[1:]] for x in self.data]\n",
    "        \n",
    "        self.input_size = model_configs[model]['input_size']\n",
    "        self.input_mean = model_configs[model]['input_mean']\n",
    "        self.input_std = model_configs[model]['input_std']\n",
    "        \n",
    "        self.transforms = transforms.Compose([GroupRandomRotate(360),\n",
    "                                              GroupScale(self.input_size),\n",
    "                                              Stack(roll=False),\n",
    "                                              ToTorchFormatTensor(div=True),\n",
    "                                              transforms.Normalize(self.input_mean, self.input_std),\n",
    "                                            ])\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_image_stack(self, image_id):\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "        absolute_paths = [\"data/train/{}_{}.png\".format(image_id, color) for color in colors]\n",
    "        images = [Image.open(path).convert('L') for path in absolute_paths]\n",
    "        \n",
    "        return images\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image_id = self.imgs[i]\n",
    "        image = self.load_image_stack(image_id)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        label = self.labels[i]\n",
    "        label_arr = np.zeros(28, dtype = np.float32)\n",
    "        [np.put(label_arr, x, 1) for x in label]\n",
    "        \n",
    "        label_arr = torch.from_numpy(label_arr)\n",
    "        \n",
    "        return image, label_arr, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct RGBY Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rgby_model(model):\n",
    "    modules = list(model.modules())\n",
    "    first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "    conv_layer = modules[first_conv_idx]\n",
    "    container = modules[first_conv_idx - 1]\n",
    "\n",
    "    params = [x.clone() for x in conv_layer.parameters()]\n",
    "    kernel_size = params[0].size()\n",
    "    new_kernel_size = kernel_size[:1] + (4, ) + kernel_size[2:]\n",
    "    new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "\n",
    "    new_conv = nn.Conv2d(4, conv_layer.out_channels,\n",
    "                         conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                         bias=True if len(params) == 2 else False)\n",
    "    \n",
    "    new_conv.weight.data = new_kernels\n",
    "    if len(params) == 2:\n",
    "        new_conv.bias.data = params[1].data \n",
    "    layer_name = list(container.state_dict().keys())[0][:-7] \n",
    "\n",
    "    setattr(container, layer_name, new_conv)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/ipykernel_launcher.py:101: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/saa2/pytorch_new/lib/python3.5/site-packages/ipykernel_launcher.py:112: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Iteration [1/1289], Loss: 4.9748 (4.9748), Elapsed Time 0.6135\n",
      "Epoch [1], Iteration [101/1289], Loss: 1.1832 (1.7922), Elapsed Time 60.7006\n",
      "Epoch [1], Iteration [201/1289], Loss: 1.3538 (1.5581), Elapsed Time 121.1066\n",
      "Epoch [1], Iteration [301/1289], Loss: 1.3143 (1.4547), Elapsed Time 180.0261\n",
      "Epoch [1], Iteration [401/1289], Loss: 1.0614 (1.3953), Elapsed Time 239.0473\n",
      "Epoch [1], Iteration [501/1289], Loss: 0.7716 (1.3460), Elapsed Time 302.0660\n",
      "Epoch [1], Iteration [601/1289], Loss: 0.9868 (1.3081), Elapsed Time 367.2882\n",
      "Epoch [1], Iteration [701/1289], Loss: 1.2413 (1.2738), Elapsed Time 434.2209\n",
      "Epoch [1], Iteration [801/1289], Loss: 1.0426 (1.2431), Elapsed Time 508.3093\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0bcfe485290c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mtrain_and_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0bcfe485290c>\u001b[0m in \u001b[0;36mtrain_and_val\u001b[0;34m(model_name, split, batch_size, epochs, lr, start_epoch)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0bcfe485290c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, log_loss, focal_loss, epoch)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfocal_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_arrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pytorch_new/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0bcfe485290c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minvprobs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def f1_micro(y_true, y_preds, thresh=0.5, eps=1e-20):\n",
    "    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n",
    "    truepos = preds_bin * y_true\n",
    "    \n",
    "    p = truepos.sum() / (preds_bin.sum() + eps) # take sums and calculate precision on scalars\n",
    "    r = truepos.sum() / (y_true.sum() + eps) # take sums and calculate recall on scalars\n",
    "    \n",
    "    f1 = 2*p*r / (p+r+eps) # we calculate f1 on scalars\n",
    "    return f1\n",
    "\n",
    "def f1_macro(y_true, y_preds, thresh=0.5, eps=1e-20):\n",
    "    preds_bin = y_preds > thresh # binary representation from probabilities (not relevant)\n",
    "    truepos = preds_bin * y_true\n",
    "\n",
    "    p = truepos.sum(axis=0) / (preds_bin.sum(axis=0) + eps) # sum along axis=0 (classes)\n",
    "                                                            # and calculate precision array\n",
    "    r = truepos.sum(axis=0) / (y_true.sum(axis=0) + eps)    # sum along axis=0 (classes) \n",
    "                                                            #  and calculate recall array\n",
    "\n",
    "    f1 = 2*p*r / (p+r+eps) # we calculate f1 on arrays\n",
    "    return np.mean(f1)\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        if not (target.size() == input.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(target.size(), input.size()))\n",
    "\n",
    "        max_val = (-input).clamp(min=0)\n",
    "        loss = input - input * target + max_val + \\\n",
    "            ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\n",
    "        invprobs = torch.nn.functional.logsigmoid(-input * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        return loss.sum(dim=1).mean()\n",
    "\n",
    "def train_and_val(model_name, split, batch_size, epochs, lr, start_epoch):\n",
    "    \n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes = 1000)\n",
    "    model = construct_rgby_model(model)\n",
    "            \n",
    "    num_features = model.last_linear.in_features \n",
    "    model.last_linear = torch.nn.Linear(num_features, 28)\n",
    "    \n",
    "    if glob.glob('{}_rgby_focal_0*'.format(model_name)):\n",
    "        pth_file = torch.load('{}_rgby_focal_0.pth.tar'.format(model_name))\n",
    "        state_dict = pth_file['state_dict']\n",
    "        model.load_state_dict(state_dict)\n",
    "        start_epoch = pth_file['epoch']\n",
    "        \n",
    "    model.cuda()\n",
    "\n",
    "    train_dataset = AtlasData(split = split, train = True, model = model_name)\n",
    "    val_dataset = AtlasData(split = split, train = False, model = model_name)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)\n",
    "    \n",
    "    log_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    focal_loss = FocalLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)   \n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch,epochs+1):\n",
    "        \n",
    "        train(model, train_loader, optimizer, log_loss, focal_loss, epoch)\n",
    "        avg_loss, f1_score = validate(model, val_loader, log_loss, focal_loss, epoch)\n",
    "                    \n",
    "        if epoch % 10 == 0:\n",
    "            filename = '{}_rgby_focal_{}_{}.pth.tar'.format(model_name, split, epoch)\n",
    "        else:\n",
    "            filename = '{}_rgby_focal_{}.pth.tar'.format(model_name, split)\n",
    "            \n",
    "        state = {'loss': avg_loss, 'f1_score': f1_score, 'epoch': epoch+1, 'state_dict': model.state_dict()}           \n",
    "        torch.save(state, filename)\n",
    "\n",
    "    \n",
    "def train(model, train_loader, optimizer, log_loss, focal_loss, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    losses = []\n",
    "    for i, (images, label_arrs, labels) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        label_arrs = label_arrs.cuda()\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = focal_loss(outputs, label_arrs)\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        \n",
    "        if i%100==0:\n",
    "            print(\"Epoch [{}], Iteration [{}/{}], Loss: {:.4f} ({:.4f}), Elapsed Time {:.4f}\"\n",
    "                .format(epoch, i+1, len(train_loader), loss.data[0], sum(losses)/len(losses), elapsed))\n",
    "            \n",
    "    print(\"Average Loss: {}\".format(sum(losses)/len(losses)))\n",
    "            \n",
    "\n",
    "def validate(model, val_loader, log_loss, focal_loss, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    y_pred = np.zeros(len(val_loader) * 28).reshape(len(val_loader), 28)\n",
    "    y_true = np.zeros(len(val_loader) * 28).reshape(len(val_loader), 28)\n",
    "\n",
    "    for i, (images, label_arrs, labels) in enumerate(val_loader):\n",
    "        images = images.cuda()\n",
    "        label_arrs_cuda = label_arrs.cuda()\n",
    "\n",
    "        raw_predictions = model(images)\n",
    "        outputs = raw_predictions.data\n",
    "        \n",
    "        loss = focal_loss(outputs, label_arrs_cuda) \n",
    "        losses.append(loss.data)\n",
    "        \n",
    "        predictions = np.arange(28)[raw_predictions.data[0] > 0.15]\n",
    "        \n",
    "        y_pred[i,:] = predictions\n",
    "        y_true[i,:] = label_arrs\n",
    "        \n",
    "        if sum(predictions) == 0:\n",
    "            prediction = np.argmax(raw_predictions.detach().cpu().numpy())\n",
    "            predictions = np.zeros(28)\n",
    "            np.put(predictions, prediction, 1)\n",
    "        \n",
    "        \n",
    "        if i%1000==0:\n",
    "            print('Testing {}/{}: Loss {}'.format(i, \n",
    "                                                 len(val_loader), \n",
    "                                                 sum(losses)/len(losses)))\n",
    "                                                                         \n",
    "    score = f1_macro(y_true, y_pred)\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    print(\"Avg Loss {}\".format(avg_loss))\n",
    "    print(\"Score {}\".format(score))\n",
    "\n",
    "    return avg_loss, score\n",
    "\n",
    "split = 0\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "\n",
    "model_list = ['xception',\n",
    "              'nasnetamobile',\n",
    "              'resnet34', \n",
    "              'senet154',\n",
    "              'polynet',]\n",
    "\n",
    "start_epoch = 1\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(\"Training {}\".format(model_name))\n",
    "    train_and_val(model_name, split, batch_size, epochs, lr, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pretrainedmodels.__dict__['polynet']()\n",
    "\n",
    "print(model.mean)\n",
    "print(model.std)\n",
    "print(model.input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalAtlasData(Dataset):\n",
    "    def __init__(self, model = 'bninception'):\n",
    "        self.image_ids = sorted(set([x.split('_')[0] for x in os.listdir('data/test')]))\n",
    "        \n",
    "        self.input_size = model_configs[model]['input_size']\n",
    "        self.input_mean = model_configs[model]['input_mean']\n",
    "        self.input_std = model_configs[model]['input_std']\n",
    "        \n",
    "        self.transforms = transforms.Compose([transforms.Resize(self.input_size),\n",
    "                                              transforms.ToTensor(),\n",
    "                                              transforms.Normalize(self.input_mean, self.input_std),\n",
    "                                            ])\n",
    "        \n",
    "        \n",
    "    def load_image_stack(self, image_id):\n",
    "        colors = ['red', 'green', 'blue', 'yellow']\n",
    "        absolute_paths = [\"data/test/{}_{}.png\".format(image_id, color) for color in colors]\n",
    "        \n",
    "        images = [skimage.io.imread(path) for path in absolute_paths]\n",
    "        \n",
    "        image_red = images[0]\n",
    "        image_green = images[1] + (images[3]/2).astype(np.uint8)\n",
    "        image_blue = images[2] + (images[3]/2).astype(np.uint8)\n",
    "        \n",
    "        final_image = np.stack((image_red, image_green, image_blue), -1)\n",
    "        to_display = Image.fromarray(final_image)\n",
    "        return to_display\n",
    "    \n",
    "    def dump_image(self, i):\n",
    "        image = self.load_image_stack(self.image_ids[i])\n",
    "        image_name = \"data/test/{}_{}.png\".format(self.image_ids[i], 'stacked')\n",
    "        image.save(image_name)\n",
    "        print(\"Saved\", image_name)\n",
    "        \n",
    "    def load_image(self, i):\n",
    "        image_id = self.image_ids[i]\n",
    "        image_path = \"data/test/{}_{}.png\".format(image_id, 'stacked')\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        return image, image_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        image, image_id = self.load_image(i)\n",
    "        image = self.transforms(image)\n",
    "        \n",
    "        return image_id, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pretrainedmodels\n",
    "import pandas as pd\n",
    "from torch.nn import Softmax\n",
    "\n",
    "def generate_preds(model_name):\n",
    "    model = pretrainedmodels.__dict__[model_name](num_classes = 1000, pretrained = 'imagenet')\n",
    "    in_features = model.last_linear.in_features\n",
    "    model.last_linear = torch.nn.Linear(in_features, 28)\n",
    "    \n",
    "    if model_name == 'polynet':\n",
    "        model = torch.nn.DataParallel(model, device_ids = [0,1,2,3]).cuda()\n",
    "        model.load_state_dict(torch.load('{}_0.pth.tar'.format(model_name)))\n",
    "        model = model.eval()\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('{}_0.pth.tar'.format(model_name)))\n",
    "        model = model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "    eval_data = EvalAtlasData(model = model_name)\n",
    "    dataloader = DataLoader(eval_data, 1, False)\n",
    "    \n",
    "    preds = []\n",
    "    for i, (image_id, images) in enumerate(dataloader):\n",
    "        images = images.cuda()\n",
    "\n",
    "        raw_predictions = (model(images))\n",
    "        predictions = np.argwhere(raw_predictions.data[0] > 0.15)\n",
    "        try:\n",
    "            num_predictions = len(predictions.data[0])\n",
    "        except IndexError:\n",
    "            num_predictions = 0\n",
    "\n",
    "        print('-----------------------------------------------------')\n",
    "        print(image_id[0])\n",
    "        print('Raw Prediction', raw_predictions)\n",
    "        if num_predictions == 0:\n",
    "            print('No value passed the threshold')\n",
    "            predictions = [np.argmax(raw_predictions.detach().cpu().numpy())]\n",
    "            num_predictions = 1\n",
    "            print(\"Prediction:\", predictions)\n",
    "            print(\"Number of predictions\", num_predictions)\n",
    "        else:\n",
    "            predictions = predictions.data[0].tolist()\n",
    "            print(\"Prediction:\", predictions)\n",
    "            print(\"Number of predictions\", num_predictions)\n",
    "\n",
    "        predicted = ' '.join('%d' % prediction for prediction in predictions)\n",
    "        print(image_id[0])\n",
    "        print(predicted)\n",
    "        pred = dict(Id = image_id[0], Predicted = predicted)\n",
    "        preds.append(pred)\n",
    "        \n",
    "    df = pd.DataFrame(preds)\n",
    "    df.to_csv('{}.csv'.format(model_name), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. Use Focal Loss https://www.kaggle.com/iafoss/pretrained-resnet34-with-rgby-fast-ai\n",
    "2. Somehow use the Y channel\n",
    "\n",
    "Trained:\n",
    "1. InceptionV4 100 epochs\n",
    "2. SE-ResNext 33 epochs\n",
    "3. PolyNet 33 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG:\n\tUnexpected key(s) in state_dict: \"_features.0.weight\", \"_features.0.bias\", \"_features.1.weight\", \"_features.1.bias\", \"_features.1.running_mean\", \"_features.1.running_var\", \"_features.3.weight\", \"_features.3.bias\", \"_features.4.weight\", \"_features.4.bias\", \"_features.4.running_mean\", \"_features.4.running_var\", \"_features.7.weight\", \"_features.7.bias\", \"_features.8.weight\", \"_features.8.bias\", \"_features.8.running_mean\", \"_features.8.running_var\", \"_features.10.weight\", \"_features.10.bias\", \"_features.11.weight\", \"_features.11.bias\", \"_features.11.running_mean\", \"_features.11.running_var\", \"_features.14.weight\", \"_features.14.bias\", \"_features.15.weight\", \"_features.15.bias\", \"_features.15.running_mean\", \"_features.15.running_var\", \"_features.17.weight\", \"_features.17.bias\", \"_features.18.weight\", \"_features.18.bias\", \"_features.18.running_mean\", \"_features.18.running_var\", \"_features.20.weight\", \"_features.20.bias\", \"_features.21.weight\", \"_features.21.bias\", \"_features.21.running_mean\", \"_features.21.running_var\", \"_features.23.weight\", \"_features.23.bias\", \"_features.24.weight\", \"_features.24.bias\", \"_features.24.running_mean\", \"_features.24.running_var\", \"_features.27.weight\", \"_features.27.bias\", \"_features.28.weight\", \"_features.28.bias\", \"_features.28.running_mean\", \"_features.28.running_var\", \"_features.30.weight\", \"_features.30.bias\", \"_features.31.weight\", \"_features.31.bias\", \"_features.31.running_mean\", \"_features.31.running_var\", \"_features.33.weight\", \"_features.33.bias\", \"_features.34.weight\", \"_features.34.bias\", \"_features.34.running_mean\", \"_features.34.running_var\", \"_features.36.weight\", \"_features.36.bias\", \"_features.37.weight\", \"_features.37.bias\", \"_features.37.running_mean\", \"_features.37.running_var\", \"_features.40.weight\", \"_features.40.bias\", \"_features.41.weight\", \"_features.41.bias\", \"_features.41.running_mean\", \"_features.41.running_var\", \"_features.43.weight\", \"_features.43.bias\", \"_features.44.weight\", \"_features.44.bias\", \"_features.44.running_mean\", \"_features.44.running_var\", \"_features.46.weight\", \"_features.46.bias\", \"_features.47.weight\", \"_features.47.bias\", \"_features.47.running_mean\", \"_features.47.running_var\", \"_features.49.weight\", \"_features.49.bias\", \"_features.50.weight\", \"_features.50.bias\", \"_features.50.running_mean\", \"_features.50.running_var\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75f7c9ca9e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg19_bn_0_40.pth.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/pytorch_new/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 721\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tUnexpected key(s) in state_dict: \"_features.0.weight\", \"_features.0.bias\", \"_features.1.weight\", \"_features.1.bias\", \"_features.1.running_mean\", \"_features.1.running_var\", \"_features.3.weight\", \"_features.3.bias\", \"_features.4.weight\", \"_features.4.bias\", \"_features.4.running_mean\", \"_features.4.running_var\", \"_features.7.weight\", \"_features.7.bias\", \"_features.8.weight\", \"_features.8.bias\", \"_features.8.running_mean\", \"_features.8.running_var\", \"_features.10.weight\", \"_features.10.bias\", \"_features.11.weight\", \"_features.11.bias\", \"_features.11.running_mean\", \"_features.11.running_var\", \"_features.14.weight\", \"_features.14.bias\", \"_features.15.weight\", \"_features.15.bias\", \"_features.15.running_mean\", \"_features.15.running_var\", \"_features.17.weight\", \"_features.17.bias\", \"_features.18.weight\", \"_features.18.bias\", \"_features.18.running_mean\", \"_features.18.running_var\", \"_features.20.weight\", \"_features.20.bias\", \"_features.21.weight\", \"_features.21.bias\", \"_features.21.running_mean\", \"_features.21.running_var\", \"_features.23.weight\", \"_features.23.bias\", \"_features.24.weight\", \"_features.24.bias\", \"_features.24.running_mean\", \"_features.24.running_var\", \"_features.27.weight\", \"_features.27.bias\", \"_features.28.weight\", \"_features.28.bias\", \"_features.28.running_mean\", \"_features.28.running_var\", \"_features.30.weight\", \"_features.30.bias\", \"_features.31.weight\", \"_features.31.bias\", \"_features.31.running_mean\", \"_features.31.running_var\", \"_features.33.weight\", \"_features.33.bias\", \"_features.34.weight\", \"_features.34.bias\", \"_features.34.running_mean\", \"_features.34.running_var\", \"_features.36.weight\", \"_features.36.bias\", \"_features.37.weight\", \"_features.37.bias\", \"_features.37.running_mean\", \"_features.37.running_var\", \"_features.40.weight\", \"_features.40.bias\", \"_features.41.weight\", \"_features.41.bias\", \"_features.41.running_mean\", \"_features.41.running_var\", \"_features.43.weight\", \"_features.43.bias\", \"_features.44.weight\", \"_features.44.bias\", \"_features.44.running_mean\", \"_features.44.running_var\", \"_features.46.weight\", \"_features.46.bias\", \"_features.47.weight\", \"_features.47.bias\", \"_features.47.running_mean\", \"_features.47.running_var\", \"_features.49.weight\", \"_features.49.bias\", \"_features.50.weight\", \"_features.50.bias\", \"_features.50.running_mean\", \"_features.50.running_var\". "
     ]
    }
   ],
   "source": [
    "import pretrainedmodels\n",
    "import torch\n",
    "\n",
    "model = pretrainedmodels.__dict__['vgg19_bn']()\n",
    "in_features = model.last_linear.in_features\n",
    "\n",
    "model.last_linear = torch.nn.Linear(in_features, 28)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('vgg19_bn_0_40.pth.tar')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
